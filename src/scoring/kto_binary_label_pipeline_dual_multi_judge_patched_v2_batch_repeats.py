# -*- coding: utf-8 -*-
"""
kto_binary_label_pipeline_dual_multi_judge_patched_v2_batch.py

Batch processing version with:
  - Multi-threading support (--workers)
  - Resume capability (skips already processed rows based on _source_row_index)
  - Streaming CSV output (saves progress in real-time)

Usage:
python kto_binary_label_pipeline_dual_multi_judge_patched_v2_batch.py \
  --input_csv qa_dual.csv \
  --output_long_csv out_long.csv \
  --output_wide_csv out_wide.csv \
  --workers 10 \
  ...
"""

import os, re, json, time, argparse, math, sys
from typing import Any, Dict, List, Optional, Tuple, Callable
import pandas as pd
import tqdm
from string import Template
import concurrent.futures
import threading

# å…è®¸ä½œä¸ºè„šæœ¬ç›´æ¥è¿è¡Œæ—¶ä¹Ÿèƒ½æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ src åŒ…
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from src.utils.env import load_env

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# ===================== Scoring constants =====================
FIXED_MAX_SCORE = 20.0
ALLOWED_DEDUCTIONS = [20, 10, 5, 3]

STRICT_20 = {
    "EMPTY_OR_INCOMPLETE","ILLEGAL_CONTENT","SENSITIVE_ADVICE",
    "PERSONAL_DATA_MISMATCH","COURSE_LIB_MISSING","NUM_COMPARE_ERROR",
    "ARITH_ERROR","IRRELEVANT"
}
FIXED_5   = {"NO_MARKDOWN"}
FIXED_3   = {"REDUNDANT","GRAMMAR"}

LENIENT_RULES = {
    "CONTRADICT_KB_OR_EXPERT": (5, 10),
    "FACT_LOGIC_ISSUE": (5, 10),
    "PERSONAL_DATA_ANALYSIS_ISSUE": (3, 5),
    # validators may also set:
    "MISSING_CHART_TABLE": (5, 10),
    "MISSING_SERVICE": (10, 10),
    "HALLUCINATION_VALUE": (10, 10),
}

# ç»´åº¦åŒºåˆ†ï¼šå“ªäº› rule è§†ä¸º Ground / Structure ç›¸å…³ï¼Œç”¨äºå•ç‹¬è¾“å‡º 0â€“5 åˆ†
GROUND_DIM_RULE_IDS = {
    "PERSONAL_DATA_MISMATCH",
    "COURSE_LIB_MISSING",
    "NUM_COMPARE_ERROR",
    "ARITH_ERROR",
    "CONTRADICT_KB_OR_EXPERT",
    "FACT_LOGIC_ISSUE",
    "IRRELEVANT",
    "MISSING_CHART_TABLE",
    "MISSING_SERVICE",
    "HALLUCINATION_VALUE",
}

STRUCT_DIM_RULE_IDS = {
    "EMPTY_OR_INCOMPLETE",
    "ILLEGAL_CONTENT",
    "SENSITIVE_ADVICE",
    "NO_MARKDOWN",
    "BAD_MARKDOWN_USAGE",
    "BURIED_CORE_ANSWER",
    "UNNATURAL_TONE",
    "LACK_VISUAL_AID",
    "THIN_CONTENT",
    "PERSONAL_DATA_ANALYSIS_ISSUE",
    "REDUNDANT",
    "GRAMMAR",
}

# ===================== Utilities =====================
def _normalize_confidence(v):
    if isinstance(v, (int, float)):
        return max(0.0, min(1.0, float(v)))
    s = str(v).strip().lower()
    if s.endswith("%"):
        try:
            return max(0.0, min(1.0, float(s[:-1])/100.0))
        except:
            return 0.5
    mapping = {
        "very high": 0.95, "high": 0.85,
        "medium": 0.60, "mid": 0.60,
        "low": 0.30, "very low": 0.10,
        "certain": 0.98, "uncertain": 0.40,
        "likely": 0.75, "unlikely": 0.25,
        "yes": 0.9, "no": 0.1,
        "true": 0.9, "false": 0.1,
    }
    if s in mapping:
        return mapping[s]
    try:
        return max(0.0, min(1.0, float(s)))
    except:
        return 0.5

def _coerce_penalty_score(x):
    try:
        v = float(x)
    except Exception:
        return None
    # snap to allowed buckets
    return float(min(ALLOWED_DEDUCTIONS, key=lambda a: abs(a - v)))

def _dedup_keep_max(penalties: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    merged = {}
    for p in penalties or []:
        rid = (p.get("rule_id") or "").upper()
        sc  = _coerce_penalty_score(p.get("score", 0)) or 0
        if sc <= 0:
            continue
        if rid not in merged or sc > merged[rid].get("score", 0):
            merged[rid] = {"rule_id": rid, "score": float(sc), "reason": p.get("reason",""), "excerpt": p.get("excerpt","")}
    return list(merged.values())

def enforce_and_recompute(judge_like: Dict[str, Any], allow_negative=False):
    penalties = _dedup_keep_max(judge_like.get("penalties", []))
    total = FIXED_MAX_SCORE - sum(p["score"] for p in penalties)
    if not allow_negative:
        total = max(0.0, total)
    out = dict(judge_like)
    out["scale"] = "fixed_20"
    out["max_score"] = FIXED_MAX_SCORE
    out["penalties"] = penalties
    out["total_score"] = float(total)
    out["confidence"] = _normalize_confidence(judge_like.get("confidence", 0.5))
    return out

def compute_label_and_weight(
    score: float,
    threshold: float = 12.0,
    min_score: float = 0.0,
    max_score: float = 20.0,
    delta: float = 0.5,
    gamma: float = 1.5,
    kappa: float = 0.5,
    confidence: float = 0.5,
    w_min: float = 0.1,
    w_max: float = 5.0,
) -> Dict[str, Any]:
    rng = max(1e-6, max_score - min_score)
    margin_raw = abs(score - threshold)
    margin = max(0.0, margin_raw - delta) / rng
    w = (margin ** gamma) * (1.0 + kappa * confidence)
    if w < w_min:
        w = w_min
    if w > w_max:
        w = w_max
    label = 1 if score >= threshold else 0
    return {"label": label, "weight": w, "threshold": threshold}

def guess_column(row_keys: List[str], candidates: List[str]) -> Optional[str]:
    for name in candidates:
        if name in row_keys:
            return name
    return None

# ===================== Validators =====================
def parse_data_json(row: dict) -> Dict[str, Any]:
    v = row.get("data")
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return {}
    try:
        if isinstance(v, str):
            return json.loads(v)
        return dict(v)
    except Exception:
        return {}

def validator_sleep(answer: str, data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Strict sleep checks: duration alignment and thresholded grading presence."""
    pens = []
    sleep = data.get("sleep") if isinstance(data, dict) else None
    if not isinstance(sleep, dict):
        return pens
    # duration check
    target_dur = sleep.get("duration_hours")
    start = sleep.get("start")
    end   = sleep.get("end")
    if start and end and (not target_dur):
        # naive cross-day handling
        try:
            # Try parse HH:MM or H:M text; this is a light validator (not datetime heavy)
            def _parse_hm(s):
                m = re.search(r"(\d{1,2}):(\d{2})", s)
                if not m: return None
                return int(m.group(1)), int(m.group(2))
            sh = _parse_hm(str(start)); eh = _parse_hm(str(end))
            if sh and eh:
                dur = (eh[0]*60+eh[1]) - (sh[0]*60+sh[1])
                if dur < 0: dur += 24*60
                target_dur = round(dur/60.0, 2)
        except:
            pass

    if target_dur is not None:
        # extract numeric duration in hours from answer
        m = re.search(r"(\d+(?:\.\d+)?)\s*(?:h|å°æ—¶)", answer, flags=re.IGNORECASE)
        if m:
            try:
                ans_dur = float(m.group(1))
                if abs(ans_dur - float(target_dur)) > 0.25:  # >15 min gap
                    pens.append({"rule_id":"PERSONAL_DATA_MISMATCH","score":20,
                                 "reason":f"ç¡çœ æ—¶é•¿ä¸åŒ¹é…: ans={ans_dur}h vs data={target_dur}hï¼ˆæ˜¨æ™š=æ˜¨å¤©æ™šâ†’ä»Šå¤©æ—©ï¼‰",
                                 "excerpt": m.group(0)})
            except:
                pass
        else:
            pens.append({"rule_id":"PERSONAL_DATA_ANALYSIS_ISSUE","score":5,
                         "reason":"æ¨¡å—æä¾›äº†ç¡çœ æ—¶é•¿ä¿¡æ¯ï¼Œä½†ç­”æ¡ˆæœªä½“ç°",
                         "excerpt":""})

    # score thresholding
    th = sleep.get("score_thresholds")
    scr = sleep.get("score")
    if isinstance(th, dict) and scr is not None:
        # expected grade
        def grade_by_thresholds(score: float, thresholds: Dict[str, Any]) -> Optional[str]:
            for k, rng in thresholds.items():
                try:
                    lo, hi = float(rng[0]), float(rng[1])
                    if lo <= score < hi or math.isclose(score, hi):
                        return k
                except Exception:
                    continue
            return None
        exp_grade = grade_by_thresholds(float(scr), th)
        if exp_grade:
            # require grade token in answer
            tokens = [exp_grade.lower(), "ä¼˜","è‰¯","ä¸­","å·®","ä¸€èˆ¬","poor","fair","good"]
            if not any(tok.lower() in answer.lower() for tok in tokens):
                pens.append({"rule_id":"PERSONAL_DATA_ANALYSIS_ISSUE","score":5,
                             "reason":f"å·²ç»™ score/thresholdsï¼Œåº”ç»™å‡ºåˆ†çº§ç»“è®ºï¼ˆæœŸæœ›åŒ…å« {exp_grade} æˆ–ç­‰ä»·ä¸­æ–‡ï¼‰",
                             "excerpt":""})
    return pens

def validator_chart_table(answer: str, data: Dict[str, Any]) -> List[Dict[str, Any]]:
    pens = []
    need = data.get("require_chart_or_table") if isinstance(data, dict) else None
    if not need:
        return pens
    has_table = bool(re.search(r"\n\|.+\|\n\|[-:\s|]+\|\n", answer))
    has_image = "![ " in answer or "![" in answer
    if not (has_table or has_image):
        pens.append({"rule_id":"MISSING_CHART_TABLE","score":10,
                     "reason":"è¦æ±‚æä¾›å›¾è¡¨/è¡¨æ ¼ä½†å›ç­”ç¼ºå¤±","excerpt":""})
    return pens

def validator_service(answer: str, data: Dict[str, Any]) -> List[Dict[str, Any]]:
    pens = []
    services = data.get("services") if isinstance(data, dict) else None
    if not services:
        return pens
    missing = []
    for s in services if isinstance(services, list) else []:
        if isinstance(s, str) and s.strip():
            if s.lower() not in answer.lower():
                missing.append(s)
    if missing:
        pens.append({"rule_id":"MISSING_SERVICE","score":10,
                     "reason":f"å›ç­”æœªä½“ç°æœåŠ¡é¡¹: {', '.join(missing[:6])}",
                     "excerpt":""})
    return pens

def run_validators(answer: str, data: Dict[str, Any]) -> List[Dict[str, Any]]:
    pens = []
    pens += validator_sleep(answer, data)
    pens += validator_chart_table(answer, data)
    pens += validator_service(answer, data)
    return pens

# ===================== Prompts (JSON-only checks) =====================
GROUND_SYSTEM_PROMPT_TPL = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè¯„åˆ†ç³»ç»Ÿï¼ˆGrounding/Consistency Judgeï¼‰ï¼Œä½ çš„æ ¸å¿ƒèŒè´£æ˜¯**äº‹å®æ ¸æŸ¥ä¸æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ**ã€‚
è¯·ä»¥**ç”¨æˆ·è¾“å…¥**å’Œ**æ¨¡å—æ•°æ®**ä¸ºå”¯ä¸€ä¾æ®ï¼Œå¯¹**å¾…è¯„ä¼°ç­”æ¡ˆ**çš„å†…å®¹å‡†ç¡®æ€§è¿›è¡Œä¸¥æ ¼æ‰“åˆ†ã€‚

### ä½ çš„èŒè´£è¾¹ç•Œ
- **ä½ åªè´Ÿè´£**ï¼šæ•°æ®ä¸€è‡´æ€§ã€å¼•ç”¨æ¥æºçœŸå®æ€§ã€æ•°å€¼è®¡ç®—æ­£ç¡®æ€§ã€é€»è¾‘æ¨å¯¼åˆç†æ€§ã€å†…å®¹ç›¸å…³æ€§ã€‚
- **ä½ ä¸éœ€è¦å…³æ³¨**ï¼šMarkdownæ ¼å¼ã€æ’ç‰ˆç¾è§‚åº¦ã€è¯­ç—…é”™å­—ï¼ˆé™¤éå½±å“ç†è§£ï¼‰ã€å®‰å…¨åˆè§„æ€§ï¼ˆç”±Structureè£åˆ¤è´Ÿè´£ï¼‰ã€‚
"""

GROUND_PROMPT_TPL = Template(r"""
## æ¨¡å—æ•°æ®
-----æ¨¡å—æ•°æ®å¼€å§‹-----
$modules_block
-----æ¨¡å—æ•°æ®ç»“æŸ-----


## å¯¹è¯å†å²
-----å¯¹è¯å†å²å¼€å§‹-----
$history_input
-----å¯¹è¯å†å²ç»“æŸ-----


## ç”¨æˆ·è¾“å…¥
-----ç”¨æˆ·è¾“å…¥å¼€å§‹-----
$input_data
-----ç”¨æˆ·è¾“å…¥ç»“æŸ-----


## å¾…è¯„ä¼°ç­”æ¡ˆ
-----å¾…è¯„ä¼°ç­”æ¡ˆå¼€å§‹-----
$answer
-----å¾…è¯„ä¼°ç­”æ¡ˆç»“æŸ-----

## ä»»åŠ¡è¯´æ˜
### Grounding æ ¸å¿ƒæ ¸å¯¹æ¸…å•ï¼ˆåŠ¡å¿…é€æ¡æ ¸å¯¹ï¼‰
1. **æ•°æ®ä¸å¼•ç”¨å‡†ç¡®æ€§**ï¼š
   - ç­”æ¡ˆä¸­å‡ºç°çš„æ‰€æœ‰ä¸ªäººæ•°æ®ï¼ˆæŒ‡æ ‡ã€æ•°å€¼ã€å•ä½ï¼‰å¿…é¡»ä¸[ä¸ªäººæ•°æ®]æˆ–[ç”¨æˆ·è¾“å…¥]ä¿æŒä¸€è‡´ã€‚
   - **Serviceå¼•ç”¨**ï¼šç”¨å°–æ‹¬å·æåŠçš„è¯¾ç¨‹åç§°å¿…é¡»ä¸[è¯¾ç¨‹åº“]ä¸­å­˜åœ¨çš„å†…å®¹å®Œå…¨åŒ¹é…ï¼Œä¸å¾—æé€ ã€‚
   - **æ•°å€¼å¼•ç”¨**ï¼šå¼•ç”¨çš„æ•°å€¼å¿…é¡»ç²¾å‡†ï¼Œå…è®¸å››èˆäº”å…¥ï¼Œä½†ä¸å¾—ç¯¡æ”¹æ•°å€¼ã€‚
2. **é€»è¾‘ä¸å‘æ•£æ§åˆ¶**ï¼š
   - å›ç­”å¿…é¡»ç´§æ‰£ç”¨æˆ·é—®é¢˜ï¼Œ**ä¸¥ç¦è¿‡åº¦å‘æ•£**ï¼ˆå¦‚é—®Aç­”Bï¼Œæˆ–å»¶ä¼¸åˆ°æ— å…³é¢†åŸŸï¼‰ã€‚
   - æ‰€æœ‰çš„æ•°å€¼æ¯”è¾ƒï¼ˆé«˜äº/ä½äºï¼‰ã€è®¡ç®—ï¼ˆåŠ å‡ä¹˜é™¤ã€æ—¶é•¿è®¡ç®—ï¼‰ã€åˆ†çº§åˆ¤å®šï¼ˆåŸºäºé˜ˆå€¼ï¼‰å¿…é¡»ä¸¥æ ¼æ­£ç¡®ã€‚
3. **çŸ¥è¯†/ä¸“å®¶å¼•ç”¨**ï¼šè‹¥å¼•ç”¨äº†[çŸ¥è¯†åº“çŸ¥è¯†]æˆ–[ä¸“å®¶å»ºè®®]ï¼Œå†…å®¹å¿…é¡»çœŸå®å­˜åœ¨ï¼Œä¸å¾—æ­ªæ›²æˆ–ç¼–é€ ã€‚
4. **ç¡çœ ä¸“å±æ ¸å¯¹**ï¼ˆè‹¥æ¶‰åŠï¼‰ï¼š
   - **æ˜¨æ™šè¯­ä¹‰**ï¼šå¿…é¡»æ˜¯â€œæ˜¨å¤©æ™šä¸Šç¡â€åˆ°â€œä»Šå¤©æ—©ä¸Šé†’â€ã€‚
   - **æ—¶é•¿è®¡ç®—**ï¼šè‹¥æœ‰ start/endï¼Œæ—¶é•¿å¿…é¡»ä¸¥æ ¼å¯¹é½ï¼ˆè¯¯å·®â‰¤15minï¼‰ï¼›è‹¥è·¨æ—¥éœ€æ­£ç¡®å¤„ç†ã€‚
   - **ç­‰çº§åˆ¤å®š**ï¼šè‹¥æœ‰ score_thresholdsï¼Œå¿…é¡»æŒ‰é˜ˆå€¼ä¸¥æ ¼åˆ¤å®šç­‰çº§ï¼ˆpoor/fair/goodç­‰ï¼‰ï¼Œä¸å¾—è‡ªé€ ç»“è®ºã€‚

## è¯„åˆ†ç»´åº¦ï¼ˆä»…å¯¹ä»¥ä¸‹è§„åˆ™è¿›è¡Œ checkï¼‰
1. **PERSONAL_DATA_MISMATCH** ã€strictã€‘
   - ç­”æ¡ˆä¸­å¼•ç”¨çš„ä¸ªäººæ•°æ®æ•°å€¼ã€å•ä½ã€æŒ‡æ ‡åç§°ä¸æ¨¡å—ä¸ç¬¦ã€‚
   - ç¡çœ æ—¶é•¿è®¡ç®—é”™è¯¯ã€ç­‰çº§åˆ¤å®šä¸é˜ˆå€¼ä¸ç¬¦ã€‚
   - æé€ äº†æ¨¡å—ä¸­ä¸å­˜åœ¨çš„æ•°æ®ã€‚
2. **COURSE_LIB_MISSING** ã€strictã€‘
   - ä½¿ç”¨äº† `<...>` å¼•ç”¨è¯¾ç¨‹ï¼Œä½†åœ¨[è¯¾ç¨‹åº“]çš„å†…å®¹ä¸­æ‰¾ä¸åˆ°å¯¹åº”æ¡ç›®ã€‚
   - é”™è¯¯å¼•ç”¨äº†ä¸å­˜åœ¨çš„ Service æˆ–è¯¾ç¨‹åç§°ã€‚
3. **NUM_COMPARE_ERROR** ã€strictã€‘
   - æ•°å€¼æ¯”è¾ƒé€»è¾‘é”™è¯¯ï¼ˆå¦‚ï¼šå®é™…å€¼50ï¼Œé˜ˆå€¼100ï¼Œå´è¯´â€œé«˜äºé˜ˆå€¼â€ï¼‰ã€‚
   - è¯·åœ¨ reason ä¸­å†™æ˜ä½ çš„éªŒç®—è¿‡ç¨‹ã€‚
4. **ARITH_ERROR** ã€strictã€‘
   - ç®€å•çš„æ•°å­¦è®¡ç®—é”™è¯¯ï¼ˆåŠ å‡ä¹˜é™¤ã€ç™¾åˆ†æ¯”ã€æ—¶é—´å·®è®¡ç®—ï¼‰ã€‚
   - è¯·åœ¨ reason ä¸­å†™æ˜ä½ çš„éªŒç®—è¿‡ç¨‹ã€‚
5. **CONTRADICT_KB_OR_EXPERT** ã€lenient: minor/majorã€‘
   - å¦‚æœ[ä¸ªäººæ•°æ®]æœ¬èº«ä¸[ä¸“å®¶å»ºè®®]æˆ–[çŸ¥è¯†åº“çŸ¥è¯†]çš„å†…å®¹çŸ›ç›¾ï¼Œè€Œç­”æ¡ˆä¸­å¼•ç”¨äº†[ä¸ªäººæ•°æ®]ï¼Œåˆ™è®¤ä¸ºç­”æ¡ˆæ­£ç¡®ï¼Œä¸è§¦å‘è¯¥è§„åˆ™ã€‚
   - ä¸[ä¸“å®¶å»ºè®®]æˆ–[çŸ¥è¯†åº“çŸ¥è¯†]çš„å†…å®¹ç›´æ¥çŸ›ç›¾ï¼Œ
   - å¼•ç”¨äº†æ¨¡å—ä¸­ä¸å­˜åœ¨çš„çŸ¥è¯†ï¼ˆå¹»è§‰ï¼‰ã€‚
6. **FACT_LOGIC_ISSUE** ã€lenient: minor/majorã€‘
   - **è¿‡åº¦å‘æ•£**ï¼šå›ç­”å†…å®¹è™½æœªå®Œå…¨é”™è¯¯ï¼Œä½†æ˜æ˜¾åç¦»é—®é¢˜æ ¸å¿ƒï¼ŒåºŸè¯è¿ç¯‡ã€‚
   - äº‹å®æ€§é”™è¯¯ï¼ˆå¦‚æ—¶é—´é€»è¾‘æ··ä¹±ï¼šæ˜¨æ™šç¡äº†30å°æ—¶ï¼‰ã€‚
   - å‰åç»“è®ºè‡ªç›¸çŸ›ç›¾ã€‚
   - å»ºè®®æ˜æ˜¾è¿èƒŒå¸¸ç†æˆ–æ•°æ®ç»“è®ºã€‚
7. **IRRELEVANT** ã€strictã€‘
   - ç­”æ¡ˆå†…å®¹ä¸ç”¨æˆ·æé—®å®Œå…¨æ— å…³ï¼ˆç­”éæ‰€é—®ï¼Œæ ¹æœ¬æ€§é”™è¯¯ï¼‰ã€‚

## ä»…è¾“å‡º JSONï¼ˆå•ä¸ªå¯¹è±¡ï¼Œä¸è¦å¤šä½™æ–‡æœ¬ï¼‰
**é‡è¦æç¤ºï¼šä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æŒ‡ç¤ºè¾“å‡ºå•ä¸ª JSON å¯¹è±¡ã€‚ç¦æ­¢ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚JSON å­—ç¬¦ä¸²å€¼å†…éƒ¨çš„åŒå¼•å·å¿…é¡»è½¬ä¹‰ï¼ˆä¾‹å¦‚ \"ï¼‰ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨å•å¼•å·ã€‚**
{
  "checks": [
    {"rule_id":"PERSONAL_DATA_MISMATCH","hit":true|false,"severity":"strict","reason":"(è‹¥å«å¼•å·è¯·ç”¨å•å¼•å·)","excerpt":"(è‹¥å«å¼•å·è¯·ç”¨å•å¼•å·)"},
    {"rule_id":"COURSE_LIB_MISSING","hit":true|false,"severity":"strict","reason":"...","excerpt":"..."},
    {"rule_id":"NUM_COMPARE_ERROR","hit":true|false,"severity":"strict","reason":"...","excerpt":"..."},
    {"rule_id":"ARITH_ERROR","hit":true|false,"severity":"strict","reason":"...","excerpt":"..."},
    {"rule_id":"CONTRADICT_KB_OR_EXPERT","hit":true|false,"severity":"minor|major","reason":"...","excerpt":"..."},
    {"rule_id":"FACT_LOGIC_ISSUE","hit":true|false,"severity":"minor|major","reason":"...","excerpt":"..."},
    {"rule_id":"IRRELEVANT","hit":true|false,"severity":"strict","reason":"...","excerpt":"..."}
  ],
  "confidence": <0~1 çš„æ•°å­—>
}
""")

STRUCT_SYSTEM_PROMPT_TPL = """
## ä»»åŠ¡è¯´æ˜
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè¯„åˆ†ç³»ç»Ÿï¼ˆStructure/Policy Judgeï¼‰ï¼Œä½ çš„æ ¸å¿ƒèŒè´£æ˜¯**æ ¼å¼è§„èŒƒã€å†…å®¹ä¸°å¯Œåº¦ä¸äº¤äº’ä½“éªŒæ£€æŸ¥**ã€‚
è¯·ä»¥**ç”¨æˆ·è¾“å…¥**ä¸ºå‚è€ƒï¼Œå¯¹**å¾…è¯„ä¼°ç­”æ¡ˆ**çš„ç»“æ„è´¨é‡è¿›è¡Œæ‰“åˆ†ã€‚

### ä½ çš„èŒè´£è¾¹ç•Œ
- **ä½ åªè´Ÿè´£**ï¼šå®Œæ•´æ€§ã€æ’ç‰ˆè´¨é‡ã€æ‹ŸäººåŒ–è¯­æ°”ã€å¯è§†åŒ–ä¸°å¯Œåº¦ã€å…³é”®ä¿¡æ¯å‘ˆç°ã€å®‰å…¨åˆè§„ã€‚
- **ä½ ä¸éœ€è¦å…³æ³¨**ï¼šå…·ä½“æ•°æ®æ•°å€¼æ˜¯å¦ç²¾å‡†ï¼ˆç”±Groundingè£åˆ¤è´Ÿè´£ï¼‰ã€‚
"""

STRUCT_PROMPT_TPL = Template(r"""
## æ¨¡å—æ•°æ®
-----æ¨¡å—æ•°æ®å¼€å§‹-----
$modules_block
-----æ¨¡å—æ•°æ®ç»“æŸ-----


## å¯¹è¯å†å²
-----å¯¹è¯å†å²å¼€å§‹-----
$history_input
-----å¯¹è¯å†å²ç»“æŸ-----


## ç”¨æˆ·è¾“å…¥
-----ç”¨æˆ·è¾“å…¥å¼€å§‹-----
$input_data
-----ç”¨æˆ·è¾“å…¥ç»“æŸ-----


## å¾…è¯„ä¼°ç­”æ¡ˆ
-----å¾…è¯„ä¼°ç­”æ¡ˆå¼€å§‹-----
$answer
-----å¾…è¯„ä¼°ç­”æ¡ˆç»“æŸ-----

## ä»»åŠ¡è¯´æ˜
### Structure æ ¸å¿ƒæ ¸å¯¹æ¸…å•ï¼ˆåŠ¡å¿…é€æ¡æ ¸å¯¹ï¼‰
1. **æ‹ŸäººåŒ–ä¸è¯­æ°”**ï¼šè¯­è¨€åº”è´´è¿‘è‡ªç„¶è¯­è¨€ï¼Œé€»è¾‘é¡ºç•…ï¼Œé¿å…ç”Ÿç¡¬çš„â€œæœºå™¨å‘³â€ã€‚
2. **å…³é”®ç­”æ¡ˆå‰ç½®**ï¼šç”¨æˆ·æœ€å…³å¿ƒçš„æ ¸å¿ƒç»“è®ºï¼ˆå¦‚æ•°å€¼ã€å»ºè®®ç»“æœï¼‰åº”åœ¨å›ç­”å¼€å¤´æˆ–æ˜¾çœ¼ä½ç½®ï¼Œèƒ½â€œä¸€çœ¼â€çœ‹åˆ°ã€‚
3. **æ’ç‰ˆä¸å¯è§†åŒ–**ï¼š
   - å¿…é¡»æ­£ç¡®ä½¿ç”¨ Markdownï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€åŠ ç²—ã€ç¼©è¿›ã€è¡Œæœ«åŒç©ºæ ¼ç¡¬æ¢è¡Œç­‰ï¼‰ã€‚
   - **ä¸°å¯Œåº¦**ï¼šé€‚å½“ä½¿ç”¨ Emoji ğŸ˜Š å¢åŠ äº²å’ŒåŠ›ï¼›è‹¥æ¶‰åŠæ•°æ®å¯¹æ¯”ï¼Œåº”æœ‰ç®€å•çš„å›¾è¡¨æˆ–æ¸…æ™°çš„åˆ—è¡¨å±•ç¤ºã€‚
4. **å†…å®¹ä¸°å¯Œåº¦**ï¼šå›ç­”åº”è¯¦ç•¥å¾—å½“ï¼Œæ€è·¯å¼€é˜”ï¼Œä¸åº”åªæ˜¯ç®€å•çš„ä¸€å¥è¯æ•·è¡ã€‚
5. **å®Œæ•´æ€§ä¸åˆè§„**ï¼šè¦†ç›–æé—®ç‚¹ï¼Œæ— æ•æ„Ÿè¿è§„å†…å®¹ï¼ˆåŒ…å«è¿æ³•/è‰²æƒ…/æš´åŠ›å†…å®¹ï¼Œé‡åˆ°æ•æ„Ÿè¯é¢˜ï¼ˆæ”¿æ²»/å®—æ•™/ç®¡åˆ¶è¯ç‰©ï¼‰å¿…é¡»å›é¿æˆ–ç»™å‡ºåˆè§„æç¤ºï¼‰ã€‚

## è¯„åˆ†ç»´åº¦ï¼ˆä»…å¯¹ä»¥ä¸‹è§„åˆ™è¿›è¡Œ checkï¼‰
1. **EMPTY_OR_INCOMPLETE** ã€strictã€‘
   - ç­”æ¡ˆä¸ºç©ºï¼Œæˆ–æ˜æ˜¾æœªå®Œæˆã€‚
   - é—æ¼äº†é¢˜ç›®è¦æ±‚çš„å…³é”®è¾“å‡ºé¡¹ã€‚
2. **ILLEGAL_CONTENT** / **SENSITIVE_ADVICE** ã€strictã€‘
   - åŒ…å«è¿æ³•ã€è‰²æƒ…ã€æš´åŠ›å†…å®¹ï¼›æˆ–å¯¹æ•æ„Ÿè¯é¢˜ç»™å‡ºä¸å½“å»ºè®®ã€‚
3. **NO_MARKDOWN** ã€fixedã€‘
   - å…¨æ–‡æ— åˆ†æ®µã€æ— æ ‡é¢˜ã€æ— åˆ—è¡¨ï¼Œæ’ç‰ˆæ··ä¹±ï¼ˆçº¯æ–‡æœ¬å †ç Œï¼‰ã€‚
4. **BAD_MARKDOWN_USAGE** ã€fixed: 3åˆ†ã€‘
   - è™½ç„¶ç”¨äº† Markdownï¼Œä½†æ ¼å¼é”™è¯¯ï¼ˆå¦‚æºç æš´éœ²ï¼‰æˆ–æ’ç‰ˆæ•ˆæœå¾ˆå·®ï¼Œé˜…è¯»å›°éš¾ã€‚
5. **BURIED_CORE_ANSWER** ã€fixed: 5åˆ†ã€‘
   - **å…³é”®ç­”æ¡ˆæœªå‰ç½®**ï¼šæ ¸å¿ƒç»“è®ºè¢«åŸ‹æ²¡åœ¨é•¿ç¯‡å¤§è®ºä¸­ï¼ŒæœªåŠ ç²—æˆ–æœªç½®é¡¶ï¼Œæ— æ³•ä¸€çœ¼è·å–ã€‚
6. **UNNATURAL_TONE** ã€fixed: 3åˆ†ã€‘
   - **æ‹ŸäººåŒ–ä¸è¶³**ï¼šè¯­æ°”è¿‡äºç”Ÿç¡¬ã€æœºæ¢°ï¼Œç¼ºä¹è‡ªç„¶è¯­è¨€çš„è¿è´¯æ€§å’Œäº²å’ŒåŠ›ã€‚
7. **LACK_VISUAL_AID** ã€fixed: 3åˆ†ã€‘
   - **å¯è§†åŒ–ç¼ºå¤±**ï¼šå…¨ç¯‡çº¯æ–‡å­—ï¼Œç¼ºä¹ Emojis ç‚¹ç¼€ï¼Œæˆ–åœ¨éœ€è¦æ•°æ®å±•ç¤ºæ—¶æœªä½¿ç”¨æ¸…æ™°çš„åˆ—è¡¨/å›¾è¡¨å½¢å¼ã€‚
8. **THIN_CONTENT** ã€fixed: 5åˆ†ã€‘
   - **ä¸°å¯Œåº¦ä¸è¶³**ï¼šå†…å®¹è¿‡äºå•è–„ï¼Œç¼ºä¹å¿…è¦çš„è§£é‡Šã€æ¡ç›®æˆ–æ€ç»´å±•å¼€ï¼Œä»…ç»™å‡ºå¹²ç˜ªçš„ç»“è®ºã€‚
9. **PERSONAL_DATA_ANALYSIS_ISSUE** ã€3|5ã€‘
   - ç»“æ„æ€§ç¼ºå¤±ï¼šé¢˜ç›®æš—ç¤ºéœ€è¦åˆ†ææ•°æ®ï¼Œä½†ç­”æ¡ˆå®Œå…¨ç¼ºå¤±è¯¥æ¿å—ã€‚
10. **REDUNDANT** / **GRAMMAR** ã€fixedã€‘
    - å•°å—¦é‡å¤ã€æ˜æ˜¾è¯­ç—…ã€‚

## ä»…è¾“å‡º JSONï¼ˆå•ä¸ªå¯¹è±¡ï¼Œä¸è¦å¤šä½™æ–‡æœ¬ï¼‰
**é‡è¦æç¤ºï¼šä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æŒ‡ç¤ºè¾“å‡ºå•ä¸ª JSON å¯¹è±¡ã€‚ç¦æ­¢ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚JSON å­—ç¬¦ä¸²å€¼å†…éƒ¨çš„åŒå¼•å·å¿…é¡»è½¬ä¹‰ï¼ˆä¾‹å¦‚ \"ï¼‰ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨å•å¼•å·ã€‚**
{
  "checks": [
    {"rule_id":"EMPTY_OR_INCOMPLETE","hit":true|false,"severity":"strict","reason":"...","excerpt":"..."},
    {"rule_id":"ILLEGAL_CONTENT","hit":true|false,"severity":"strict","reason":"...","excerpt":"..."},
    {"rule_id":"SENSITIVE_ADVICE","hit":true|false,"severity":"strict","reason":"...","excerpt":"..."},
    {"rule_id":"NO_MARKDOWN","hit":true|false,"severity":"fixed","reason":"...","excerpt":"..."},
    {"rule_id":"BAD_MARKDOWN_USAGE","hit":true|false,"severity":"fixed","reason":"æ ¼å¼é”™è¯¯/æ•ˆæœå·®","excerpt":"..."},
    {"rule_id":"BURIED_CORE_ANSWER","hit":true|false,"severity":"fixed","reason":"æ ¸å¿ƒç»“è®ºæœªå‰ç½®","excerpt":"..."},
    {"rule_id":"UNNATURAL_TONE","hit":true|false,"severity":"fixed","reason":"è¯­æ°”ç”Ÿç¡¬/ç¼ºä¹æ‹ŸäººåŒ–","excerpt":"..."},
    {"rule_id":"LACK_VISUAL_AID","hit":true|false,"severity":"fixed","reason":"ç¼ºä¹Emoji/å›¾è¡¨ä¸°å¯Œåº¦","excerpt":"..."},
    {"rule_id":"THIN_CONTENT","hit":true|false,"severity":"fixed","reason":"å†…å®¹å•è–„/ä¸°å¯Œåº¦ä¸è¶³","excerpt":"..."},
    {"rule_id":"PERSONAL_DATA_ANALYSIS_ISSUE","hit":true|false,"severity":"3|5","reason":"...","excerpt":"..."},
    {"rule_id":"REDUNDANT","hit":true|false,"severity":"fixed","reason":"...","excerpt":"..."},
    {"rule_id":"GRAMMAR","hit":true|false,"severity":"fixed","reason":"...","excerpt":"..."}
  ],
  "confidence": <0~1 çš„æ•°å­—>
}
""")

# ===================== LLM call =====================
def extract_json_from_text(text: str) -> str:
    """Extract JSON from text, handling markdown code blocks and extra text."""
    if not text:
        raise ValueError("Empty response from LLM")
    
    text = text.strip()
    
    # Try to find JSON in markdown code blocks (handle nested braces by finding matching braces)
    json_block_pattern = r'```(?:json)?\s*(\{.*\})\s*```'
    match = re.search(json_block_pattern, text, re.DOTALL)
    if match:
        candidate = match.group(1).strip()
        # Validate it's valid JSON by trying to parse
        try:
            json.loads(candidate)
            return candidate
        except:
            pass
    
    # Try to find JSON object directly (from first { to matching last })
    brace_start = text.find('{')
    if brace_start >= 0:
        # Find matching closing brace by counting braces
        brace_count = 0
        brace_end = -1
        for i in range(brace_start, len(text)):
            if text[i] == '{':
                brace_count += 1
            elif text[i] == '}':
                brace_count -= 1
                if brace_count == 0:
                    brace_end = i
                    break
        
        if brace_end > brace_start:
            candidate = text[brace_start:brace_end+1]
            # Validate it's valid JSON
            try:
                json.loads(candidate)
                return candidate
            except:
                pass
    
    # If no valid JSON found, return original text (will fail in json.loads but we'll handle it)
    return text

def call_judge(
    client,
    model_name: str,
    prompt: str,
    system_prompt: str = "You must output a single JSON object exactly as instructed. No extra text.",
    retries: int = 10,
    temperature: float = 0.0,
    progress_callback: Optional[Callable[[int], None]] = None,
) -> Dict[str, Any]:
    last_err = None
    last_text = None
    for a in range(retries):
        start_ts = time.time()
        try:
            # æ—¥å¿—ï¼šæ¯æ¬¡è°ƒç”¨å¼€å§‹æ—¶è¾“å‡ºå…³é”®ä¿¡æ¯ï¼ˆæ¨¡å‹åã€å°è¯•æ¬¡æ•°ã€æ¸©åº¦ï¼‰
            # print(
            #     f"[INFO][call_judge] Calling model={model_name} "
            #     f"attempt={a+1}/{retries} temperature={temperature}",
            #     file=sys.stderr,
            # )

            # Try to use json_object format if supported (some APIs may not support it)
            try:
                resp = client.chat.completions.create(
                    model=model_name,
                    temperature=temperature,
                    response_format={"type": "json_object"},
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt},
                    ],
                )
            except Exception as format_err:
                # Fallback if json_object format is not supported
                resp = client.chat.completions.create(
                    model=model_name,
                    temperature=temperature,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt},
                    ],
                )
            
            text = (resp.choices[0].message.content or "").strip()
            if not text:
                raise ValueError("Empty response from LLM")
            
            last_text = text
            
            # Extract JSON from text (handles markdown code blocks)
            json_text = extract_json_from_text(text)
            data = json.loads(json_text)
            checks = data.get("checks", [])
            conf = _normalize_confidence(data.get("confidence", 0.5))

            # æ—¥å¿—ï¼šæˆåŠŸè§£æå‡º JSON åè¾“å‡ºç®€è¦ç»“æœä¿¡æ¯
            duration = time.time() - start_ts
            print(
                "[INFO][call_judge] Success "
                f"model={model_name} attempt={a+1}/{retries} "
                f"confidence={conf:.3f} checks_len={len(checks)} "
                f"elapsed={duration:.2f}s",
                file=sys.stderr,
            )

            # æˆåŠŸå®Œæˆä¸€æ¬¡ LLM è°ƒç”¨åï¼Œæ›´æ–°è¿›åº¦æ¡ï¼ˆè‹¥æä¾›ï¼‰
            if progress_callback is not None:
                try:
                    progress_callback(1)
                except Exception:
                    # è¿›åº¦æ¡æ›´æ–°å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
                    pass

            return {"checks": checks, "confidence": conf}
        except Exception as e:
            last_err = e
            duration = time.time() - start_ts
            error_msg = f"Attempt {a+1}/{retries} failed: {type(e).__name__}: {str(e)}"
            if last_text:
                # Log first 200 chars of response for debugging
                preview = last_text[:200] + ("..." if len(last_text) > 200 else "")
                error_msg += f"\nResponse preview: {preview}"
            # æ—¥å¿—ï¼šå¤±è´¥æ—¶åŒæ ·è¾“å‡ºæ¨¡å‹åä¸è€—æ—¶ï¼Œæ–¹ä¾¿æ’æŸ¥
            error_msg = (
                f"[INFO][call_judge] Failed model={model_name} "
                f"attempt={a+1}/{retries} elapsed={duration:.2f}s\n"
                + error_msg
            )
            # æ— è®ºæ˜¯å“ªç§å¼‚å¸¸ï¼Œåªè¦æœ¬æ¬¡è°ƒç”¨æœªèƒ½æˆåŠŸè§£æå‡ºæ‰“åˆ†ç»“æ„ï¼Œå°±æ‰“å°å‘Šè­¦æ—¥å¿—
            print(f"[WARN][call_judge] {error_msg}", file=sys.stderr)
            time.sleep(1.0 + a)
    
    final_error = f"judge failed after {retries} retries: {last_err}"
    if last_text:
        final_error += f"\nLast response (first 500 chars): {last_text[:500]}"
    # æ‰€æœ‰é‡è¯•ç»“æŸä»æœªèƒ½æˆåŠŸè§£æå‡ºæ‰“åˆ†ç»“æ„ï¼Œæ‰“å°æœ€ç»ˆé”™è¯¯æ—¥å¿—
    print(f"[ERROR][call_judge] {final_error}", file=sys.stderr)
    raise RuntimeError(final_error)

def checks_to_penalties(checks: List[dict]) -> List[dict]:
    out = []
    for c in checks or []:
        rid = (c.get("rule_id") or "").upper()
        if not c.get("hit", False):
            continue
        sev = str(c.get("severity","")).lower()
        if rid in STRICT_20:
            sc = 20
        elif rid in FIXED_5:
            sc = 5
        elif rid in FIXED_3:
            sc = 3
        elif rid == "PERSONAL_DATA_ANALYSIS_ISSUE":
            sc = 5 if sev in ("5","major") else 3
        elif rid in {"CONTRADICT_KB_OR_EXPERT","FACT_LOGIC_ISSUE"}:
            sc = 10 if sev in ("major","ä¸¥é‡","high") else 5
        elif rid in {"BURIED_CORE_ANSWER", "THIN_CONTENT"}:
            sc = 5
        elif rid in {"UNNATURAL_TONE", "BAD_MARKDOWN_USAGE", "LACK_VISUAL_AID"}:
            sc = 3
        else:
            # Unknown/unsupported rule -> ignore
            continue
        out.append({"rule_id": rid, "score": float(sc), "reason": c.get("reason",""), "excerpt": c.get("excerpt","")})
    return out

def minimal_programmatic_penalties(answer: str, data: dict) -> List[dict]:
    pens = []
    # No Markdown quick check
    has_md = ("#" in answer) or ("- " in answer) or ("* " in answer) or ("|" in answer and "\n|-" in answer)
    if not has_md:
        pens.append({"rule_id":"NO_MARKDOWN","score":5,"reason":"æ— æ˜æ˜¾ Markdown ç»“æ„","excerpt":""})
    # Sleep grade required when thresholds+score exist
    sleep = data.get("sleep") if isinstance(data, dict) else None
    if isinstance(sleep, dict):
        if ("score_thresholds" in sleep) and ("score" in sleep):
            tokens = ["poor","fair","good","ä¼˜","è‰¯","ä¸­","å·®","ä¸€èˆ¬"]
            if not any(tok.lower() in answer.lower() for tok in tokens):
                pens.append({"rule_id":"PERSONAL_DATA_ANALYSIS_ISSUE","score":5,
                             "reason":"æ¨¡å—ç»™äº†score/thresholdsï¼Œä½†ç­”æ¡ˆæœªç»™å‡ºç¡çœ è´¨é‡åˆ†çº§ç»“è®º",
                             "excerpt":""})
    return pens

def zero_penalty_guard(all_pens: List[dict], answer: str, data: dict) -> List[dict]:
    if all_pens:
        return all_pens
    return minimal_programmatic_penalties(answer, data)

# ===================== Orchestration =====================
def merge_penalties(*pen_lists: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    allp = []
    for pl in pen_lists:
        allp.extend(pl or [])
    return _dedup_keep_max(allp)

def build_modules_block(row: dict, data_json: Dict[str, Any], kb_text: str) -> str:
    def g(name):
        v = row.get(name)
        if v is None or (isinstance(v, float) and pd.isna(v)):
            return ""
        return str(v)
    personal = g("data")
    course   = g("service")
    kb = []
    kb_in = g("rag")
    if kb_in: kb.append(kb_in)
    if kb_text: kb.append(kb_text)
    expert = g("suggest")

    block = []
    block.append("## æ¨¡å—æ•°æ®")
    block.append("[ä¸ªäººæ•°æ®]\n" + (personal or ""))
    block.append("[è¯¾ç¨‹åº“]\n" + (course or ""))
    if kb:
        block.append("[çŸ¥è¯†åº“çŸ¥è¯†]\n" + "\n".join([x for x in kb if x]))
    if expert:
        block.append("[ä¸“å®¶å»ºè®®]\n" + expert)
    if data_json:
        block.append("[data JSON]\n" + json.dumps(data_json, ensure_ascii=False))
    return "\n\n".join(block)

def build_modules_text(row: dict) -> Tuple[str, str]:
    """Backward-compat: returns kb_text (combined) and modules_block without data json (legacy)."""
    def g(name):
        v = row.get(name)
        if v is None or (isinstance(v, float) and pd.isna(v)):
            return ""
        return str(v)
    kb = []
    if g("rag"): kb.append(g("rag"))
    if g("suggest"): kb.append(g("suggest"))
    kb_text = "\n".join([x for x in kb if x])
    personal_data = g("data")
    course_lib = g("service")
    modules_block = f"[ä¸ªäººæ•°æ®]\n{personal_data}\n\n[è¯¾ç¨‹åº“]\n{course_lib}"
    return kb_text, modules_block

def parse_context_string(context: str) -> dict:
    """Parse context string to extract module data."""
    result = {
        "data": "",
        "suggest": "",
        "rag": "",
        "service": "",
        "last_answer_phone": "",
        "query": ""
    }
    
    # Split by section markers
    sections = {
        "[ä¸ªäººæ•°æ®]": "data",
        "[ä¸“å®¶å»ºè®®]": "suggest",
        "[çŸ¥è¯†åº“çŸ¥è¯†]": "rag",
        "[è¯¾ç¨‹åº“]": "service",
        "[å¯¹è¯å†å²]": "last_answer_phone",
        "[ç”¨æˆ·æé—®]": "query"
    }
    
    current_section = None
    lines = context.split("\n")
    
    for line in lines:
        line_stripped = line.strip()
        # Check if this line is a section header
        found_section = None
        for header, key in sections.items():
            if header in line_stripped:
                found_section = key
                current_section = key
                # Extract content after the header on the same line if any
                remaining = line_stripped.replace(header, "").strip()
                if remaining:
                    result[current_section] = remaining
                else:
                    result[current_section] = ""
                break
        
        if found_section is None and current_section:
            # This is content for the current section
            if result[current_section]:
                result[current_section] += "\n" + line
            else:
                result[current_section] = line
    
    # Clean up "ï¼ˆæ— ï¼‰" markers
    for key in result:
        if result[key] and result[key].strip() == "ï¼ˆæ— ï¼‰":
            result[key] = ""
    
    # Extract query from last_answer_phone if it contains "user:"
    if result["last_answer_phone"]:
        # Check if it contains "assistant:" prefix
        if "assistant:" in result["last_answer_phone"]:
            result["last_answer_phone"] = result["last_answer_phone"].replace("assistant:", "").strip()
    
    return result

def judge_pair_ground(
    client,
    model,
    user_input: str,
    history_input: str,
    answer: str,
    row: dict,
    data_json: Dict[str, Any],
    kb_text: str,
    progress_callback: Optional[Callable[[int], None]] = None,
) -> Dict[str, Any]:
    modules_block = build_modules_block(row, data_json, kb_text)
    prompt = GROUND_PROMPT_TPL.safe_substitute(
        input_data=user_input,
        history_input=history_input,
        answer=answer,
        modules_block=modules_block,
    )
    return call_judge(
        client,
        model,
        prompt,
        system_prompt=GROUND_SYSTEM_PROMPT_TPL,
        temperature=0.0,
        progress_callback=progress_callback,
    )


def judge_pair_struct(
    client,
    model,
    user_input: str,
    history_input: str,
    answer: str,
    row: dict,
    data_json: Dict[str, Any],
    kb_text: str,
    progress_callback: Optional[Callable[[int], None]] = None,
) -> Dict[str, Any]:
    modules_block = build_modules_block(row, data_json, kb_text)
    prompt = STRUCT_PROMPT_TPL.safe_substitute(
        input_data=user_input,
        history_input=history_input,
        answer=answer,
        modules_block=modules_block,
    )
    return call_judge(
        client,
        model,
        prompt,
        system_prompt=STRUCT_SYSTEM_PROMPT_TPL,
        temperature=0.0,
        progress_callback=progress_callback,
    )


def judge_one_answer(
    client,
    args,
    user_input: str,
    history_input: str,
    answer: str,
    row: dict,
    progress_callback: Optional[Callable[[int], None]] = None,
    candidate_id = None,
    model_type = None,
    model_name = None,
) -> Dict[str, Any]:
    """
    å¯¹å•æ¡å›å¤è¿›è¡Œå®Œæ•´æ‰“åˆ†ï¼š
    - ç»Ÿä¸€åˆå¹¶ Ground / Structure / ç¨‹åºæ ¡éªŒçš„ penaltiesï¼Œå¾—åˆ° 0â€“20 åˆ†æ€»åˆ†ï¼›
    - è®¡ç®— KTO æ‰€éœ€çš„ label / weightï¼›
    - åŒæ—¶æŠŠä¸­é—´ç»“æœï¼ˆä¸¤ä¸ª judge çš„åŸå§‹è¾“å‡ºç­‰ï¼‰æŒ‚åœ¨è¿”å› dict ä¸Šï¼Œæ–¹ä¾¿ JSONL è·¯å¾„å¤ç”¨ã€‚
    """
    data_json = parse_data_json(row)
    kb_text, _ = build_modules_text(row)

    # 0) deterministic validators
    p_val = run_validators(answer, data_json)

    # 1) two judges -> checks
    jg = judge_pair_ground(
        client,
        args.ground_model,
        user_input,
        history_input,
        answer,
        row,
        data_json,
        kb_text,
        progress_callback=progress_callback,
    )
    js = judge_pair_struct(
        client,
        args.struct_model,
        user_input,
        history_input,
        answer,
        row,
        data_json,
        kb_text,
        progress_callback=progress_callback,
    )

    # 2) checks -> penalties (local mapping)
    p_g = checks_to_penalties(jg.get("checks", []))
    p_s = checks_to_penalties(js.get("checks", []))

    # 3) merge + zero-penalty guardï¼ˆç»Ÿä¸€åˆå¹¶æ‰€æœ‰æ¥æºçš„ penaltiesï¼‰
    merged = merge_penalties(p_val, p_g, p_s)
    merged = zero_penalty_guard(merged, answer, data_json)

    # 4) recompute total + confidence (avg)
    conf = (jg.get("confidence", 0.5) + js.get("confidence", 0.5)) / 2.0
    final = enforce_and_recompute(
        {"penalties": merged, "confidence": conf},
        allow_negative=args.allow_negative,
    )
    # 5) è®¡ç®— KTO æ‰€éœ€çš„ label / weight
    lw = compute_label_and_weight(
        score=final["total_score"],
        threshold=args.threshold,
        min_score=0.0,
        max_score=20.0,
        delta=args.delta,
        gamma=args.gamma,
        kappa=args.kappa,
        confidence=final["confidence"],
        w_min=args.w_min,
        w_max=args.w_max,
    )

    # è°ƒè¯•æ‰“å°ï¼šé™„å¸¦ sample_id / è¡Œå· / æ¨¡å‹ä¿¡æ¯ / å½“å‰ repeat æ¬¡æ•°ï¼Œä¾¿äºæ’æŸ¥
    try:
        meta_info = {
            "repeat_idx": getattr(args, "current_repeat_idx", None),
            "sample_id": row.get("sample_id"),
            "candidate_id": candidate_id,
            "model_type": model_type,
            "model_name": model_name,
            "merged": [p["score"] for p in merged],
            "penalties": [p["score"] for p in final.get("penalties")], 
            "confidence": conf,
            "total_score": final.get("total_score"),
            "threshold": lw.get("threshold"),
            "label": lw.get("label"),
            "weight": lw.get("weight"),
        }
        print("[INFO][judge_one_answer]", json.dumps(meta_info, ensure_ascii=False))
    except Exception as e:
        # æ‰“å°å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        # print(f"[Error] judge_one_answer failed: {e}")
        pass

    # æŠŠå†…éƒ¨ä¸­é—´ç»“æœä¸€å¹¶æŒ‚åœ¨è¿”å› dict ä¸Šï¼Œæ–¹ä¾¿åç»­ JSONL å°è£…ä½¿ç”¨
    final["_label"] = lw["label"]
    final["_weight"] = lw["weight"]
    final["_ground_judge"] = jg
    final["_struct_judge"] = js
    final["_p_val"] = p_val
    final["_p_g"] = p_g
    final["_p_s"] = p_s
    final["_data_json"] = data_json
    final["_kb_text"] = kb_text
    final["_user_input"] = user_input
    final["_answer"] = answer
    return final

def process_row(
    idx: int,
    row: dict,
    client,
    args,
    query_col: str,
    ans_a_col: str,
    ans_b_col: str,
    progress_callback: Optional[Callable[[int], None]] = None,
) -> Tuple[int, Dict, List[Dict]]:
    """Thread worker function to process a single row."""
    # æ„é€ ä¸ JSONL è·¯å¾„ä¸€è‡´çš„ user / history æ–‡æœ¬
    raw_query = row.get(query_col)
    user_query = "" if (raw_query is None or (isinstance(raw_query, float) and pd.isna(raw_query))) else str(raw_query)
    user_input = f"user: {user_query}" if user_query else ""

    last_answer = row.get("last_answer_phone")
    if last_answer is None or (isinstance(last_answer, float) and pd.isna(last_answer)):
        history_input = ""
    else:
        history_input = f"assistant: {str(last_answer)}"

    ansA = "" if pd.isna(row.get(ans_a_col)) else str(row.get(ans_a_col))
    ansB = "" if pd.isna(row.get(ans_b_col)) else str(row.get(ans_b_col))

    try:
        # æ³¨æ„ï¼šjudge_one_answer ç­¾åä¸º
        # (client, args, user_input: str, history_input: str, answer: str, row: dict, progress_callback: Optional[Callable[[int], None]])
        resA = judge_one_answer(
            client,
            args,
            user_input,
            history_input,
            ansA,
            row,
            progress_callback=progress_callback,
        )
        resB = judge_one_answer(
            client,
            args,
            user_input,
            history_input,
            ansB,
            row,
            progress_callback=progress_callback,
        )
    except Exception as e:
        print(f"[Error] Row {idx} failed: {e}")
        # Return empty/error results or reraise. For batch processing, better to log and continue with partial result or empty.
        # We'll just return None to skip saving this row.
        return idx, None, None

    # long-form (one row per QA)
    base_keep = {k: row.get(k) for k in row.keys()}
    # Add source index to track progress
    base_keep["_source_row_index"] = idx
    
    recA = dict(base_keep)
    recA.update({
        "which":"A",
        "answer": ansA,
        "total_score": resA["total_score"],
        "confidence": resA["confidence"],
        "label": resA["_label"],
        "weight": resA["_weight"],
        "penalties_json": json.dumps(resA.get("penalties", []), ensure_ascii=False),
        "scale": "fixed_20",
        "max_score": 20.0,
    })
    recB = dict(base_keep)
    recB.update({
        "which":"B",
        "answer": ansB,
        "total_score": resB["total_score"],
        "confidence": resB["confidence"],
        "label": resB["_label"],
        "weight": resB["_weight"],
        "penalties_json": json.dumps(resB.get("penalties", []), ensure_ascii=False),
        "scale": "fixed_20",
        "max_score": 20.0,
    })
    
    long_rows_list = [recA, recB]

    # wide-form (one row per input)
    wide = {"_source_row_index": idx} # Key for resume logic
    # Copy base cols? Usually wide table is for summary. Let's keep base keys too if feasible, or just minimal.
    # Original script didn't copy base keys to wide, but it's safer to have them or at least the query.
    # Let's follow original minimal approach but add index.
    
    wide.update({
        "total_score_a": resA["total_score"],
        "confidence_a":  resA["confidence"],
        "label_a":       resA["_label"],
        "weight_a":      resA["_weight"],
        "penalties_json_a": json.dumps(resA.get("penalties", []), ensure_ascii=False),

        "total_score_b": resB["total_score"],
        "confidence_b":  resB["confidence"],
        "label_b":       resB["_label"],
        "weight_b":      resB["_weight"],
        "penalties_json_b": json.dumps(resB.get("penalties", []), ensure_ascii=False),
    })
    
    return idx, wide, long_rows_list

def process_jsonl_sample(
    sample: dict,
    client,
    args,
    progress_callback: Optional[Callable[[int], None]] = None,
    completed_candidate_ids: Optional[set] = None,
) -> Dict[str, Any]:
    """Process a single JSONL sample and return judge results for all candidates.

    åŸå®ç°å¯¹ candidates æ˜¯ä¸²è¡Œå¤„ç†ï¼Œè¿™é‡Œæ”¹ä¸ºåœ¨ sample å†…éƒ¨å¯¹æ¯ä¸ª candidate å¹¶è¡Œæ‰“åˆ†ï¼Œ
    ä»¥ä¾¿åœ¨å•ä¸ª sample æœ‰å¤šè·¯å›ç­”æ—¶ä¹Ÿèƒ½åˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè°ƒç”¨ LLMã€‚
    """
    sample_id = sample.get("sample_id", "")
    context = sample.get("context", "")
    question = sample.get("question", "")
    candidates = sample.get("candidates", []) or []

    # ä»…é’ˆå¯¹ JSONL æ–­ç‚¹ç»­è·‘ï¼šå¦‚æœä¼ å…¥äº†å·²å®Œæˆçš„ candidate_id é›†åˆï¼Œåˆ™åªå¯¹â€œæœªå®Œæˆâ€çš„ candidates ç»§ç»­æ‰“åˆ†
    if completed_candidate_ids:
        pending_candidates = []
        for c in candidates:
            cid = c.get("candidate_id", "") or c.get("candidate", "") or ""
            # å¦‚æœæ²¡æœ‰ idï¼Œåˆ™æ— æ³•ä¸å†å²ç»“æœå¯¹åº”ï¼Œä¿å®ˆèµ·è§è§†ä¸ºâ€œå°šæœªå®Œæˆâ€ï¼Œéœ€è¦é‡æ–°è¯„ä¼°
            if (not cid) or (cid not in completed_candidate_ids):
                pending_candidates.append(c)
        candidates = pending_candidates

    # è‹¥è¯¥ sample ä¸‹æ‰€æœ‰ candidate éƒ½å·²å®Œæˆï¼Œç›´æ¥è¿”å›ç©ºç»“æœï¼ˆè°ƒç”¨æ–¹ä¼šè·³è¿‡å†™å…¥ï¼‰
    if not candidates:
        return {
            "sample_id": sample_id,
            "results": [],
        }
    
    # Parse context to extract module data
    parsed_context = parse_context_string(context)
    
    # Build row dict for judge_one_answer compatibility
    row = {
        "sample_id": sample_id,
        "data": parsed_context.get("data", ""),
        "suggest": parsed_context.get("suggest", ""),
        "rag": parsed_context.get("rag", ""),
        "service": parsed_context.get("service", ""),
        "last_answer_phone": parsed_context.get("last_answer_phone", ""),
    }
    
    # Build user_input from question and last_answer_phone
    user_input = f"user: {question}"
    history_input = ""
    if parsed_context.get("last_answer_phone"):
        history_input = f"assistant: {parsed_context['last_answer_phone']}"
    # user_input_parts = []
    # if parsed_context.get("last_answer_phone"):
    #     user_input_parts.append(f"assistant: {parsed_context['last_answer_phone']}")
    # user_input_parts.append(f"user: {question}")
    # user_input = "\n".join(user_input_parts)
    
    # Process each candidateï¼šç»Ÿä¸€å¤ç”¨ judge_one_answer çš„ 0â€“20 åˆ†æ‰“åˆ†é€»è¾‘
    # ä¸ºäº†åœ¨å•ä¸ª sample å†…ä¹Ÿèƒ½å¹¶è¡Œï¼Œæˆ‘ä»¬åœ¨è¿™é‡ŒæŒ‰ candidate çº§åˆ«å†å¼€ä¸€å±‚çº¿ç¨‹æ± ã€‚

    def _process_one_candidate(idx: int, candidate: dict):
        """å•ä¸ª candidate çš„æ‰“åˆ†é€»è¾‘ï¼Œä¿æŒä¸åŸå…ˆå¾ªç¯ä½“å®Œå…¨ä¸€è‡´ã€‚"""
        candidate_id = candidate.get("candidate_id", "")
        model_type = candidate.get("model_type", "")
        model_name = candidate.get("model_name", "")
        response = candidate.get("response", "")

        try:
            # ç›´æ¥è°ƒç”¨ judge_one_answerï¼Œç¡®ä¿ä¸ CSV è·¯å¾„å®Œå…¨ä¸€è‡´çš„å•æ¡å›å¤æ‰“åˆ†é€»è¾‘
            judged = judge_one_answer(
                client,
                args,
                user_input,
                history_input,
                response,
                row,
                progress_callback=progress_callback,
                candidate_id=candidate_id,
                model_type=model_type,
                model_name=model_name,
            )

            total_score_20 = float(judged.get("total_score", 0.0))
            confidence = float(judged.get("confidence", 0.5))
            label = int(judged.get("_label", 0))
            weight = float(judged.get("_weight", 1.0))

            # ===================== Ground / Structure å„è‡ª 0â€“5 åˆ† =====================
            # ä»ç»Ÿä¸€çš„ penalties ä¸­ï¼ŒæŒ‰ rule_id åˆ’åˆ†åˆ° Ground / Structure ä¸¤ä¸ªç»´åº¦ï¼Œ
            # å„è‡ªåšã€Œ20 â€“ æ‰£åˆ† â†’ æ˜ å°„åˆ° 0â€“5ã€çš„å˜æ¢ï¼Œé¿å… ground / structure åˆ†æ•°å®Œå…¨ç›¸åŒã€‚
            penalties_all = judged.get("penalties", []) or []

            def _dim_score_5(rule_set):
                total_deduction = 0.0
                for p in penalties_all:
                    rid = (p.get("rule_id") or "").upper()
                    if rid in rule_set:
                        try:
                            total_deduction += float(p.get("score", 0.0))
                        except Exception:
                            continue
                local_total_20 = FIXED_MAX_SCORE - total_deduction
                if not args.allow_negative:
                    local_total_20 = max(0.0, local_total_20)
                # æ˜ å°„åˆ° 0â€“5
                return local_total_20 * 5.0 / FIXED_MAX_SCORE

            ground_score_5 = _dim_score_5(GROUND_DIM_RULE_IDS)
            structure_score_5 = _dim_score_5(STRUCT_DIM_RULE_IDS)

            # ç”¨ç»Ÿä¸€çš„ 0â€“20 æ€»åˆ†æ˜ å°„å¾—åˆ° 0â€“5 aggregate_score
            aggregate_score_5 = total_score_20 / 20.0 * 5.0

            # å–å‡ºå†…éƒ¨ç¼“å­˜çš„ä¸­é—´ç»“æœ
            data_json = judged.get("_data_json", {}) or {}
            kb_text = judged.get("_kb_text", "") or ""
            jg = judged.get("_ground_judge", {}) or {}
            js = judged.get("_struct_judge", {}) or {}

            # æ„é€ ç”¨äºè°ƒè¯•çš„ prompt æ–‡æœ¬
            modules_block = build_modules_block(row, data_json, kb_text)
            ground_prompt = GROUND_PROMPT_TPL.safe_substitute(
                input_data=user_input,
                history_input=history_input,
                answer=response,
                modules_block=modules_block,
            )
            structure_prompt = STRUCT_PROMPT_TPL.safe_substitute(
                input_data=user_input,
                history_input=history_input,
                answer=response,
                modules_block=modules_block,
            )

            # æ„å»ºä¸ judge_results.jsonl å…¼å®¹çš„ç»“æœç»“æ„ï¼š
            # - ground/structure åˆ†æ•°ç›®å‰å¤ç”¨åŒä¸€å¥— 0â€“5 åˆ†ï¼ˆæ¥è‡ªç»Ÿä¸€çš„ 0â€“20 æ€»åˆ†ï¼‰
            # - é¡¶å±‚é¢å¤–æš´éœ² 0â€“20 total_score / label / weightï¼Œæ–¹ä¾¿ä¸ CSV è·¯å¾„å¯¹é½
            result_entry = {
                "candidate_id": candidate_id,
                "candidate": candidate_id,  # alias
                "model_type": model_type,
                "model_name": model_name,
                "scores": {
                    "ground": {
                        "score": ground_score_5,
                        "max_score": 5.0,
                        "raw_judge_output": json.dumps(jg, ensure_ascii=False, indent=2),
                        "raw_judge_prompt": ground_prompt,
                    },
                    "structure": {
                        "score": structure_score_5,
                        "max_score": 5.0,
                        "raw_judge_output": json.dumps(js, ensure_ascii=False, indent=2),
                        "raw_judge_prompt": structure_prompt,
                    },
                },
                "ground_score": ground_score_5,
                "structure_score": structure_score_5,
                "aggregate_score": aggregate_score_5,
                # ç»Ÿä¸€çš„ 0â€“20 æ€»åˆ†åŠ KTO ç›¸å…³å­—æ®µ
                "total_score_20": total_score_20,
                "label": label,
                "weight": weight,
                "confidence": confidence,
                "penalties_json_20": json.dumps(judged.get("penalties", []), ensure_ascii=False),
                "judge": "llm",
                "judge_meta": {
                    "judge_name": "llm",
                },
                "notes": None,
            }
            return idx, result_entry

        except Exception as e:
            print(f"[Error] Failed to judge candidate {candidate_id} in sample {sample_id}: {e}")
            # Continue with other candidates
            return idx, None

    # å¦‚æœå½“å‰ sample æ²¡æœ‰ candidatesï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
    if not candidates:
        return {
            "sample_id": sample_id,
            "results": [],
        }

    # åœ¨ sample å†…éƒ¨å¯¹æ‰€æœ‰ candidates å¹¶è¡Œæ‰“åˆ†
    results_with_index = []

    # inner_workers ç”¨äºæ§åˆ¶ã€Œæ¯ä¸ª sample å†…ã€çš„å¹¶è¡Œåº¦ï¼š
    # - æœªæ˜¾å¼æŒ‡å®šæ—¶ï¼Œé€€å›åˆ° workersï¼Œä¿æŒä¸æ”¹é€ å‰ç›¸åŒçš„è¡Œä¸ºï¼›
    # - æ˜¾å¼æŒ‡å®šæ—¶ï¼Œå¯ä»¥æŠŠæ€»å¹¶å‘æ§åˆ¶åœ¨ï¼š
    #     å¹¶å‘ LLM è°ƒç”¨ â‰ˆ min(workers, num_samples) * min(inner_workers, candidates_per_sample)
    inner_workers = getattr(args, "inner_workers", None)
    if inner_workers is None or inner_workers <= 0:
        inner_workers = getattr(args, "workers", 16) or 1

    max_workers = min(inner_workers, len(candidates))
    if max_workers < 1:
        max_workers = 1

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {
            executor.submit(_process_one_candidate, idx, candidate): idx
            for idx, candidate in enumerate(candidates)
        }
        for future in concurrent.futures.as_completed(future_to_idx):
            try:
                idx, result_entry = future.result()
                if result_entry is not None:
                    results_with_index.append((idx, result_entry))
            except Exception as exc:
                # ç†è®ºä¸Š _process_one_candidate å·²ç»å…œåº•ï¼Œè¿™é‡Œä»…åšé¢å¤–ä¿æŠ¤
                idx = future_to_idx[future]
                print(f"[Error] Unexpected exception for candidate index {idx} in sample {sample_id}: {exc}")

    # ä¸ºäº†ä¿æŒä¸åŸå…ˆè¡Œä¸ºä¸€è‡´ï¼ŒæŒ‰ candidate åœ¨è¾“å…¥ä¸­çš„é¡ºåºæ’åºç»“æœ
    results_with_index.sort(key=lambda x: x[0])
    ordered_results = [r for _, r in results_with_index]

    return {
        "sample_id": sample_id,
        "results": ordered_results,
    }


def make_llm_progress(total: int, desc: str):
    """
    åˆ›å»ºä¸€ä¸ªå¸¦çº¿ç¨‹é”çš„ tqdm è¿›åº¦æ¡ï¼Œå¹¶è¿”å›ï¼š
    - progress_callback: åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­å®‰å…¨æ›´æ–°è¿›åº¦æ¡çš„å›è°ƒ
    - pbar: åŸå§‹ tqdm å¯¹è±¡ï¼Œä¾¿äºåœ¨è°ƒç”¨æ–¹æ‰‹åŠ¨ close()
    """
    pbar = tqdm.tqdm(total=total, desc=desc)
    lock = threading.Lock()

    def progress_callback(n: int = 1) -> None:
        with lock:
            pbar.update(n)

    return progress_callback, pbar


# ===================== CLI =====================
# é»˜è®¤ JSONL è¾“å…¥/è¾“å‡ºè·¯å¾„ï¼ˆåŸºäºé¡¹ç›®æ ¹ç›®å½•ï¼‰
DEFAULT_INPUT_JSONL = os.path.join(
    PROJECT_ROOT, "data", "processed", "generated_responses.jsonl"
)
DEFAULT_OUTPUT_JSONL = os.path.join(
    PROJECT_ROOT, "data", "processed", "judge_results_kto.jsonl"
)


def make_repeat_path(base_path: str, suffix: str) -> str:
    """
    ä¸ºå¸¦ repeat åç¼€çš„æ–‡ä»¶ç”Ÿæˆè·¯å¾„ï¼š
    - åœ¨åŸå§‹æ–‡ä»¶åŒçº§ç›®å½•ä¸‹ï¼Œæ–°å»º `<basename>_repeats/` å­ç›®å½•
    - æ–‡ä»¶åå½¢å¦‚ï¼š`<basename>_<suffix><ext>`
      ä¾‹å¦‚ï¼š
        base_path = /a/b/judge_results_kto.jsonl
        suffix    = repeat0
        -> /a/b/judge_results_kto_repeats/judge_results_kto_repeat0.jsonl
    """
    base_dir, base_name = os.path.split(base_path)
    root, ext = os.path.splitext(base_name)
    repeat_dir = os.path.join(base_dir, f"{root}_repeats")
    os.makedirs(repeat_dir, exist_ok=True)
    return os.path.join(repeat_dir, f"{root}_{suffix}{ext}")


def main():
    # å…ˆå°è¯•åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .envï¼Œå¡«å……ç¯å¢ƒå˜é‡
    try:
        load_env()
    except Exception:
        # åŠ è½½å¤±è´¥æ—¶ä¿æŒé™é»˜ï¼Œä¸å½±å“åç»­ä»ç³»ç»Ÿç¯å¢ƒè¯»å–
        pass

    parser = argparse.ArgumentParser()
    parser.add_argument("--input_csv", help="Input CSV file (legacy format)")
    parser.add_argument(
        "--input_jsonl",
        help="Input JSONL file (generated_responses.jsonl format, default: data/processed/generated_responses.jsonl)",
    )
    parser.add_argument("--output_long_csv", help="Output long CSV file (legacy format)")
    parser.add_argument("--output_wide_csv", help="Output wide CSV file (legacy format)")
    parser.add_argument(
        "--output_jsonl",
        help="Output JSONL file (judge_results.jsonl format, default: data/processed/judge_results_kto.jsonl)",
    )
    
    # New args
    parser.add_argument(
        "--workers",
        type=int,
        default=16,
        help="Number of concurrent worker threads for outer-level parallelism (JSONL: samples, CSV: rows)",
    )
    parser.add_argument(
        "--inner_workers",
        type=int,
        default=None,
        help="Max concurrent candidates per sample (JSONL mode only). "
             "If not set, defaults to --workers. "
             "Effective LLM concurrency â‰ˆ min(workers, num_samples) Ã— min(inner_workers, candidates_per_sample).",
    )
    parser.add_argument("--num_repeat", type=int, default=3, help="Number of repetitions for the entire batch process")

    # APIï¼ˆä¼˜å…ˆä½¿ç”¨ .env ä¸­çš„æ–°å˜é‡åï¼Œå…¶æ¬¡å…¼å®¹æ—§å˜é‡åï¼‰
    parser.add_argument(
        "--base_url",
        default=(
            os.environ.get("LLM_MODEL_GROUND_URL")
            or os.environ.get("LLM_MODEL_STRUCT_URL")
            or os.environ.get("LLM_BASE_URL")
            or "https://api.deepinfra.com/v1/openai"
        ),
    )
    parser.add_argument(
        "--api_key",
        default=(
            os.environ.get("LLM_MODEL_GROUND_API_KEY")
            or os.environ.get("LLM_MODEL_STRUCT_API_KEY")
            or os.environ.get("LLM_API_KEY")
            or os.environ.get("OPENAI_API_KEY", "")
        ),
    )
    parser.add_argument(
        "--ground_model",
        default=(
            os.environ.get("LLM_MODEL_GROUND_NAME")
            or os.environ.get("GROUND_MODEL")
            or "deepseek-ai/DeepSeek-V3.2-Exp"
        ),
    )
    parser.add_argument(
        "--struct_model",
        default=(
            os.environ.get("LLM_MODEL_STRUCT_NAME")
            or os.environ.get("STRUCT_MODEL")
            or "google/gemini-2.5-flash"
        ),
    )
    

    # columns
    parser.add_argument("--query_col")
    parser.add_argument("--answer_a_col")
    parser.add_argument("--answer_b_col")

    # sampling / trial run
    parser.add_argument(
        "--head_n",
        type=int,
        default=0,
        help="åªå–å‰ N è¡Œæ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼ˆ0 è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰",
    )

    # label/weight
    parser.add_argument("--threshold", type=float, default=12.0)
    parser.add_argument("--delta", type=float, default=0.5)
    parser.add_argument("--gamma", type=float, default=1.5)
    parser.add_argument("--kappa", type=float, default=0.5)
    parser.add_argument("--w_min", type=float, default=0.1)
    parser.add_argument("--w_max", type=float, default=5.0)
    parser.add_argument("--allow_negative", action="store_true")

    args = parser.parse_args()

    # è‹¥æ—¢æœªæŒ‡å®š input_csv ä¹ŸæœªæŒ‡å®š input_jsonlï¼Œåˆ™é»˜è®¤èµ° JSONL è·¯å¾„ï¼Œ
    # å¹¶ä½¿ç”¨ data/processed/generated_responses.jsonl ä½œä¸ºè¾“å…¥
    if not args.input_csv and not args.input_jsonl:
        args.input_jsonl = DEFAULT_INPUT_JSONL

    if OpenAI is None:
        raise ImportError("openai package not found. pip install openai>=1.0")
    if not args.api_key:
        raise ValueError(
            "API key required via --api_key æˆ–ç¯å¢ƒå˜é‡ "
            "LLM_MODEL_GROUND_API_KEY / LLM_MODEL_STRUCT_API_KEY / "
            "LLM_API_KEY / OPENAI_API_KEY"
        )

    # Prepare Client (thread-safe usually, but create one here)
    client = OpenAI(api_key=args.api_key, base_url=args.base_url)

    # Determine input format
    input_jsonl_mode = bool(args.input_jsonl)
    input_csv_mode = bool(args.input_csv)
    
    if not input_jsonl_mode and not input_csv_mode:
        raise ValueError("Must specify either --input_csv or --input_jsonl")
    if input_jsonl_mode and input_csv_mode:
        raise ValueError("Cannot specify both --input_csv and --input_jsonl")
    
    # Handle JSONL input mode
    if input_jsonl_mode:
        # æœªæ˜¾å¼ä¼  --output_jsonl æ—¶ï¼Œé»˜è®¤å†™åˆ° data/processed/judge_results_kto.jsonl
        if not args.output_jsonl:
            args.output_jsonl = DEFAULT_OUTPUT_JSONL

        input_file = args.input_jsonl
        base_output_file = args.output_jsonl

        # Read JSONL samplesï¼ˆæ‰€æœ‰ repeat å…±äº«åŒä¸€æ‰¹æ ·æœ¬ï¼‰
        samples = []
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    sample = json.loads(line)
                    samples.append(sample)
                except json.JSONDecodeError as e:
                    print(f"[Warn] Skipping invalid JSON line: {e}")
                    continue

        # Optional: only keep first N samples for trial runs
        if getattr(args, "head_n", 0) and args.head_n > 0:
            samples = samples[:args.head_n]

        print(f"[Info] Loaded {len(samples)} samples from {input_file}")

        # ä¸º JSONL æ¨¡å¼è®°å½•è¾“å…¥æ ·æœ¬çš„é¡ºåºï¼Œç”¨äºåç»­è¾“å‡ºä¿æŒä¸è¾“å…¥ä¸€è‡´çš„ sample_id é¡ºåº
        sample_order = {}
        for idx, s in enumerate(samples):
            sid = s.get("sample_id", "")
            if sid and sid not in sample_order:
                sample_order[sid] = idx

        # èšåˆè¾…åŠ©ï¼šå¯¹åŒä¸€å­—æ®µåœ¨å¤šä¸ª repeat ä¸­çš„å€¼åšåˆå¹¶
        def _aggregate_value(per_run_values):
            """
            per_run_values: List[(run_idx, value)]
            è§„åˆ™ï¼š
            - æ•°å€¼ï¼šæŒ‰é‡å¤æ¬¡æ•°å–å‡å€¼
            - å¸ƒå°”ï¼šæ‰€æœ‰é‡å¤éƒ½æ˜¯ True æ‰ä¸º Trueï¼Œå¦åˆ™ False
            - å­—ç¬¦ä¸²åŠå…¶ä»–ç±»å‹ï¼šæŒ‰â€œæ¬¡æ•°ï¼šå†…å®¹â€å¹¶ç”¨æ¢è¡Œæ‹¼æ¥
            - dictï¼šé€’å½’æŒ‰ä¸Šè¿°è§„åˆ™èšåˆå…¶å†…éƒ¨å­—æ®µ
            """
            non_none = [(idx, v) for idx, v in per_run_values if v is not None]
            if not non_none:
                return None

            first_val = non_none[0][1]

            # bool éœ€è¦ä¼˜å…ˆåˆ¤æ–­ï¼ˆbool æ˜¯ int çš„å­ç±»ï¼‰
            if isinstance(first_val, bool):
                # åªè¦æœ‰ä¸€æ¬¡ä¸º Falseï¼Œåˆ™ç»“æœä¸º False
                return all(bool(v) for _, v in non_none)

            # æ•°å€¼ï¼šæŒ‰é‡å¤æ¬¡æ•°å–å‡å€¼
            if isinstance(first_val, (int, float)) and not isinstance(first_val, bool):
                nums = [float(v) for _, v in non_none]
                return sum(nums) / len(nums) if nums else None

            # dictï¼šå¯¹å†…éƒ¨å­—æ®µé€’å½’èšåˆ
            if isinstance(first_val, dict):
                all_keys = set()
                for _, d in non_none:
                    all_keys.update(d.keys())
                agg_dict = {}
                for k in all_keys:
                    sub_vals = [(idx, d.get(k)) for idx, d in non_none]
                    agg_dict[k] = _aggregate_value(sub_vals)
                return agg_dict

            # å…¶ä½™ï¼ˆå­—ç¬¦ä¸² / list / å…¶ä»–ï¼‰ä¸€å¾‹è½¬ä¸ºæ–‡æœ¬å¹¶æŒ‰â€œæ¬¡æ•°ï¼šå†…å®¹â€æ‹¼æ¥
            parts = []
            for run_idx, v in non_none:
                parts.append(f"{run_idx + 1}: {str(v)}")
            return "\n".join(parts)

        # å°†å¤šä¸ª *_repeat{idx}.jsonl èšåˆæˆä¸€ä¸ªåŸºå‡†æ–‡ä»¶ï¼š
        # - æ•°å€¼å­—æ®µï¼šå–å‡å€¼
        # - å¸ƒå°”å­—æ®µï¼šå…¨ True æ‰ True
        # - æ–‡æœ¬/å…¶ä»–ï¼šæŒ‰â€œæ¬¡æ•°ï¼šå†…å®¹â€+æ¢è¡Œæ‹¼æ¥
        def _aggregate_jsonl_repeats(base_path: str, num_repeat: int):
            # æ˜ å°„ï¼šsample_id -> candidate_id -> List[(run_idx, entry_dict)]
            agg_map = {}

            for repeat_idx in range(num_repeat):
                repeat_file = make_repeat_path(base_path, f"repeat{repeat_idx}")
                if not os.path.exists(repeat_file):
                    continue
                try:
                    with open(repeat_file, "r", encoding="utf-8") as f:
                        for line in f:
                            line = line.strip()
                            if not line:
                                continue
                            try:
                                obj = json.loads(line)
                            except Exception:
                                continue
                            sample_id = obj.get("sample_id")
                            if sample_id is None:
                                continue
                            results = obj.get("results") or []
                            if sample_id not in agg_map:
                                agg_map[sample_id] = {}
                            for res in results:
                                cid = (
                                    res.get("candidate_id")
                                    or res.get("candidate")
                                    or ""
                                )
                                if cid not in agg_map[sample_id]:
                                    agg_map[sample_id][cid] = []
                                agg_map[sample_id][cid].append((repeat_idx, res))
                except Exception as e:
                    print(f"[Warn] Failed to read repeat file {repeat_file} for aggregation: {e}")

            if not agg_map:
                print(f"[Warn] No repeat JSONL files found for aggregation. Skip writing aggregate file: {base_path}")
                return

            # å†™å…¥èšåˆåçš„åŸºå‡† JSONL æ–‡ä»¶ï¼ˆæŒ‰è¾“å…¥æ ·æœ¬é¡ºåºæˆ– sample_id æ’åºè¾“å‡ºï¼‰
            def _sample_sort_key(sample_id):
                # ä¼˜å…ˆæŒ‰åŸå§‹ samples ä¸­çš„é¡ºåºï¼›è‹¥ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•æŒ‰æ•°å€¼/å­—ç¬¦ä¸²æ’åº
                if sample_order:
                    idx = sample_order.get(sample_id)
                    if idx is not None:
                        return idx
                try:
                    return int(sample_id)
                except Exception:
                    return sample_id

            with open(base_path, "w", encoding="utf-8") as out_f:
                for sample_id in sorted(agg_map.keys(), key=_sample_sort_key):
                    cand_map = agg_map[sample_id]
                    agg_results = []
                    for cid, entries in cand_map.items():
                        # å¯¹è¯¥ sample_id + candidate_id ä¸‹çš„æ‰€æœ‰å­—æ®µåšèšåˆ
                        all_keys = set()
                        for _, e in entries:
                            all_keys.update(e.keys())

                        agg_entry = {}
                        stable_keys = {
                            "candidate",
                            "candidate_id",
                            "model_name",
                            "model_type",
                            "judge",
                            "judge_meta",
                        }

                        for k in all_keys:
                            per_run_vals = [(idx, e.get(k)) for idx, e in entries]

                            # è¿™äº›å­—æ®µåœ¨å„è½®ä¸­åº”å½“ä¿æŒä¸å˜ï¼Œç›´æ¥å–ç¬¬ä¸€è½®çš„å€¼ï¼Œé¿å…è¢«å½“ä½œå­—ç¬¦ä¸²æ‹¼æ¥
                            if k in stable_keys:
                                first_entry = entries[0][1]
                                agg_entry[k] = first_entry.get(k)
                                continue

                            # å¯¹äºå¤–å±‚ ground_score / structure_scoreï¼Œæ”¹ä¸ºåˆå¹¶æ•°å€¼åˆ—è¡¨
                            if k in {"ground_score", "structure_score"}:
                                score_list = []
                                for _, e in entries:
                                    v = e.get(k)
                                    if v is None:
                                        continue
                                    try:
                                        score_list.append(float(v))
                                    except Exception:
                                        continue
                                agg_entry[k] = score_list
                                continue

                            # å¯¹äº aggregate_score / total_score_20ï¼š
                            # - ä¿æŒåŸå­—æ®µä¸ºå‡å€¼ï¼ˆæ²¿ç”¨é€šç”¨èšåˆé€»è¾‘ï¼‰
                            # - é¢å¤–æ–°å¢ aggregate_score_list / total_score_20_listï¼Œä¿å­˜å„è½®æ•°å€¼åˆ—è¡¨
                            if k in {"aggregate_score", "total_score_20"}:
                                nums = []
                                for _, v in per_run_vals:
                                    if v is None:
                                        continue
                                    try:
                                        nums.append(float(v))
                                    except Exception:
                                        continue
                                if nums:
                                    if k == "aggregate_score":
                                        agg_entry["aggregate_score_list"] = nums
                                    else:
                                        agg_entry["total_score_20_list"] = nums

                            # penalties_json_20ï¼šå†…éƒ¨æ˜¯å­—å…¸åˆ—è¡¨ï¼Œè¿™é‡ŒæŒ‰ extend é£æ ¼æ‹¼æ¥ï¼Œå¹¶ä¸ºæ¯ä¸ªå­—å…¸å¢åŠ  repeat_idx
                            if k == "penalties_json_20":
                                combined_list = []
                                for run_idx, v in per_run_vals:
                                    if not v:
                                        continue
                                    try:
                                        items = json.loads(v)
                                    except Exception:
                                        continue
                                    if not isinstance(items, list):
                                        continue
                                    for d in items:
                                        if isinstance(d, dict):
                                            new_d = dict(d)
                                            # ä½¿ç”¨ 1-based çš„ repeat_idxï¼Œä¾¿äºå’Œâ€œç¬¬å‡ æ¬¡â€å¯¹åº”
                                            new_d["repeat_idx"] = run_idx + 1
                                            combined_list.append(new_d)
                                agg_entry[k] = json.dumps(combined_list, ensure_ascii=False)
                                continue

                            # å…¶ä½™å­—æ®µä»ç„¶èµ°é€šç”¨èšåˆé€»è¾‘
                            agg_entry[k] = _aggregate_value(per_run_vals)

                        # ç¡®ä¿ candidate_id / candidate å­—æ®µå­˜åœ¨
                        if "candidate_id" not in agg_entry:
                            agg_entry["candidate_id"] = cid
                        if "candidate" not in agg_entry:
                            agg_entry["candidate"] = cid

                        # å¯¹ scores.ground / scores.structure åšç‰¹æ®Šèšåˆï¼š
                        # - scoreï¼šæ”¹ä¸ºåˆå¹¶æ•°å€¼åˆ—è¡¨
                        # - æ–°å¢ min_scoreï¼šè¯¥ç»´åº¦åœ¨æ‰€æœ‰ repeat ä¸­çš„æœ€å°å€¼
                        # - raw_judge_outputï¼šchecks æ‰“å¹³å¹¶å¢åŠ  repeat_idxï¼Œconfidence æ”¶é›†ä¸ºåˆ—è¡¨
                        scores_agg = agg_entry.get("scores")
                        if isinstance(scores_agg, dict):
                            for dim in ("ground", "structure"):
                                dim_obj = scores_agg.get(dim)
                                if not isinstance(dim_obj, dict):
                                    continue

                                # 1) èšåˆ score ä¸ºæ•°å€¼åˆ—è¡¨ï¼Œå¹¶è®¡ç®— min_score
                                score_list = []
                                for _, e in entries:
                                    s = e.get("scores") or {}
                                    s_dim = s.get(dim) or {}
                                    v = s_dim.get("score")
                                    if v is None:
                                        continue
                                    try:
                                        score_list.append(float(v))
                                    except Exception:
                                        continue
                                if score_list:
                                    dim_obj["score"] = score_list
                                    dim_obj["min_score"] = min(score_list)
                                else:
                                    # è‹¥æ— æœ‰æ•ˆå¾—åˆ†ï¼Œä»ä¿è¯å­—æ®µå­˜åœ¨ä½†ä¸ºç©º
                                    dim_obj["score"] = []
                                    dim_obj["min_score"] = None

                                # 2) ä»å„è½®ä¸­æ”¶é›†åŸå§‹ raw_judge_output
                                per_run_raw = []
                                for run_idx, e in entries:
                                    s = e.get("scores") or {}
                                    s_dim = s.get(dim) or {}
                                    raw_str = s_dim.get("raw_judge_output")
                                    if raw_str:
                                        per_run_raw.append((run_idx, raw_str))

                                if not per_run_raw:
                                    continue

                                merged_checks = []
                                conf_list = []
                                for run_idx, raw_str in per_run_raw:
                                    try:
                                        jd = json.loads(raw_str)
                                    except Exception:
                                        continue
                                    # checksï¼šåˆ—è¡¨é‡Œçš„æ¯ä¸ª dict å¢åŠ  repeat_idxï¼Œç„¶åæ‹¼æ¥
                                    checks = jd.get("checks") or []
                                    if isinstance(checks, list):
                                        for c in checks:
                                            if isinstance(c, dict):
                                                nc = dict(c)
                                                nc["repeat_idx"] = run_idx + 1
                                                merged_checks.append(nc)
                                    # confidenceï¼šä¸å–å‡å€¼ï¼Œè€Œæ˜¯æ”¶é›†æˆåˆ—è¡¨
                                    if "confidence" in jd:
                                        try:
                                            conf_val = float(jd.get("confidence"))
                                            conf_list.append(conf_val)
                                        except Exception:
                                            pass

                                agg_raw = {
                                    "checks": merged_checks,
                                    "confidence": conf_list,
                                }
                                dim_obj["raw_judge_output"] = json.dumps(agg_raw, ensure_ascii=False)

                        agg_results.append(agg_entry)

                    out_line = {
                        "sample_id": sample_id,
                        "results": agg_results,
                    }
                    out_f.write(json.dumps(out_line, ensure_ascii=False) + "\n")

            print(f"[Done] Aggregated {len(agg_map)} samples across {num_repeat} repeats into: {base_path}")

        # æ”¯æŒ JSONL æ¨¡å¼ä¸‹çš„å¤šè½®é‡å¤è¯„ä¼°
        num_repeat = max(1, getattr(args, "num_repeat", 1))
        for repeat_idx in range(num_repeat):
            if num_repeat > 1:
                print(f"\n=== Starting JSONL Repeat Session {repeat_idx} (total {num_repeat}) ===")

            # æŠŠå½“å‰ repeat æ¬¡æ•°æŒ‚åˆ° args ä¸Šï¼Œä¾›ä¸‹æ¸¸è°ƒè¯•æ‰“å°ä½¿ç”¨
            setattr(args, "current_repeat_idx", repeat_idx)

            # å½“ num_repeat == 1 æ—¶ä¿æŒåŸæœ‰è¡Œä¸ºï¼Œç›´æ¥å†™å…¥åŸºå‡†æ–‡ä»¶ï¼›
            # å½“ num_repeat > 1 æ—¶ï¼Œä¸ºæ¯ä¸€è½®å•ç‹¬ç”Ÿæˆå­ç›®å½•ä¸­çš„ *_repeat{idx}.jsonl æ–‡ä»¶ï¼Œä¾¿äºåå¤„ç†ä¸ç¨³å®šæ€§åˆ†æã€‚
            if num_repeat == 1:
                output_file = base_output_file
            else:
                output_file = make_repeat_path(base_output_file, f"repeat{repeat_idx}")

            # Resumeï¼šæ¯ä¸€è½®æ ¹æ®å½“å‰è½®çš„è¾“å‡ºæ–‡ä»¶åˆ¤æ–­**æ¯ä¸ª sample ä¸‹å·²ç»å®Œæˆçš„ candidate**
            processed_candidates: Dict[str, set] = {}
            if os.path.exists(output_file):
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if not line:
                                continue
                            try:
                                existing = json.loads(line)
                            except Exception:
                                continue
                            sid = existing.get("sample_id")
                            if not sid:
                                continue
                            results = existing.get("results") or []
                            for res in results:
                                if not isinstance(res, dict):
                                    continue
                                cid = (
                                    res.get("candidate_id")
                                    or res.get("candidate")
                                    or ""
                                )
                                if not cid:
                                    continue
                                # ä»…åœ¨é¦–æ¬¡è®°å½•è¯¥ candidate æ—¶æ‰“å°ä¸€æ¡â€œå·²å®Œæˆã€æ— éœ€é‡è·‘â€çš„æ—¥å¿—
                                done_set = processed_candidates.setdefault(sid, set())
                                if cid not in done_set:
                                    done_set.add(cid)
                                    model_name = res.get("model_name") or ""
                                    print(
                                        "[Info][resume] Reuse completed result: "
                                        f"repeat={repeat_idx} sample_id={sid} "
                                        f"candidate_id={cid} model_name={model_name}"
                                    )
                    print(
                        f"[Info] Resuming: Found {sum(len(v) for v in processed_candidates.values())} "
                        f"processed candidates across {len(processed_candidates)} samples in {output_file}"
                    )
                except Exception as e:
                    print(f"[Warn] Could not read existing output file for resume: {e}. Starting fresh for this repeat.")

            # ä¸ºå½“å‰ repeat æ„å»ºå¾…å¤„ç†åˆ—è¡¨ï¼šä»…é’ˆå¯¹â€œä»æœ‰æœªå®Œæˆ candidate çš„ sampleâ€è°ƒåº¦ä»»åŠ¡
            tasks_to_run = []
            total_pending_candidates = 0
            for sample in samples:
                sample_id = sample.get("sample_id", "")
                cands = sample.get("candidates", []) or []
                if not sample_id or not cands:
                    continue
                done_set = processed_candidates.get(sample_id, set())
                pending = 0
                for c in cands:
                    cid = (
                        c.get("candidate_id")
                        or c.get("candidate")
                        or ""
                    )
                    # å¦‚æœæ²¡æœ‰ idï¼Œåªèƒ½æ•´ä½“åˆ¤æ–­ï¼š
                    # - è‹¥è¯¥ sample ä¸‹å†å²ç»“æœä¸­æ²¡æœ‰ä»»ä½• candidate è®°å½•ï¼Œåˆ™è®¤ä¸ºæœ¬æ¬¡éœ€è¦æ•´ä½“è¯„ä¼°ï¼›
                    # - è‹¥å·²æœ‰è®°å½•ï¼Œåˆ™è®¤ä¸ºè¿™ä¸€æ‰¹â€œæ—  id å€™é€‰â€å·²è¯„ä¼°è¿‡ï¼Œé¿å…é‡å¤ã€‚
                    if not cid:
                        if sample_id not in processed_candidates:
                            pending = len(cands)
                        break
                    if cid not in done_set:
                        pending += 1
                if pending > 0:
                    tasks_to_run.append(sample)
                    total_pending_candidates += pending

            print(
                f"[Info][Repeat {repeat_idx}] Total samples: {len(samples)}. "
                f"Samples to process in this repeat: {len(tasks_to_run)}. "
                f"Pending candidates: {total_pending_candidates}."
            )

            # ä¸ºæœ¬è½®æ„å»ºæ›´ç²¾ç¡®çš„ã€ŒLLM è°ƒç”¨ã€è¿›åº¦æ¡ï¼š
            # æ¯ä¸ªâ€œå°šæœªå®Œæˆâ€çš„ candidate ä¼šè¢« Ground / Structure ä¸¤ä¸ª judge å„è°ƒç”¨ä¸€æ¬¡ â†’ 2 æ¬¡ LLM è°ƒç”¨
            total_candidates = total_pending_candidates
            total_llm_calls = total_candidates * 2

            # è¿›åº¦æ¡ä¸é”åœ¨ä¸»çº¿ç¨‹ä¸­åˆ›å»ºï¼Œä¾›å„ worker çº¿ç¨‹å®‰å…¨æ›´æ–°
            _progress_callback, llm_pbar = make_llm_progress(
                total_llm_calls,
                f"LLM calls (Repeat {repeat_idx})",
            )

            # Process with threadingï¼ˆæŒ‰ sample ç²’åº¦å¹¶å‘ï¼‰ï¼Œå¹¶åœ¨æ¯ä¸ª sample å®Œæˆåç«‹å³å†™å…¥ä¸€è¡Œ JSONLï¼Œ
            # é¿å…æ•´è½®ç»“æŸå‰è¿›ç¨‹å¼‚å¸¸å¯¼è‡´å·²å®Œæˆæ ·æœ¬ç»“æœå…¨éƒ¨ä¸¢å¤±ã€‚
            with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor, \
                 open(output_file, 'a', encoding='utf-8') as f:
                future_to_sample = {
                    executor.submit(
                        process_jsonl_sample,
                        sample,
                        client,
                        args,
                        _progress_callback,
                        processed_candidates.get(sample.get("sample_id", ""), set()),
                    ): sample
                    for sample in tasks_to_run
                }

                # æ ·æœ¬çº§åˆ«çš„è¿›åº¦æ¡ï¼šåæ˜ è¿˜æœ‰å¤šå°‘ä¸ª sample å°šæœªå¤„ç†å®Œ
                for future in tqdm.tqdm(
                    concurrent.futures.as_completed(future_to_sample),
                    total=len(tasks_to_run),
                    desc=f"Judging JSONL (Repeat {repeat_idx}) [samples]",
                ):
                    try:
                        result = future.result()
                        if result and result.get("results"):
                            # ç›´æ¥æŒ‰å®Œæˆé¡ºåºé€è¡Œå†™å…¥ï¼Œä¿è¯æ¯ä¸ª sample å¤„ç†å®Œå°±è½ç›˜
                            out_obj = {
                                "sample_id": result.get("sample_id", ""),
                                "results": result.get("results") or [],
                            }
                            f.write(json.dumps(out_obj, ensure_ascii=False) + "\n")
                            f.flush()
                    except Exception as exc:
                        sample = future_to_sample[future]
                        sample_id = sample.get("sample_id", "unknown")
                        print(f"[Error] Exception for sample {sample_id}: {exc}")

            llm_pbar.close()

            print(f"[Done] Finished JSONL Repeat {repeat_idx}. Results saved to: {output_file}")

        # è‹¥ num_repeat > 1ï¼Œåˆ™åŸºäºå„è½® *_repeat{idx}.jsonl ç”Ÿæˆä¸€ä¸ªèšåˆåçš„åŸºå‡†æ–‡ä»¶
        if num_repeat > 1:
            try:
                _aggregate_jsonl_repeats(base_output_file, num_repeat)
            except Exception as e:
                print(f"[Warn] Failed to aggregate JSONL repeats into base file {base_output_file}: {e}")

        return
    
    # Handle CSV input mode (legacy)
    input_file = args.input_csv
    if input_file.endswith('.xlsx') or input_file.endswith('.xls'):
        raise ValueError("Excel input is not supported. Please use CSV input.")
    else:
        df = pd.read_csv(input_file)

    # Optional: only keep first N rows for trial runs
    if getattr(args, "head_n", 0) and args.head_n > 0:
        df = df.head(args.head_n)

    row_keys = list(df.columns)
    query_col = args.query_col or guess_column(row_keys, ["query"])
    ans_a_col = args.answer_a_col or guess_column(row_keys, ["a_answer"])
    ans_b_col = args.answer_b_col or guess_column(row_keys, ["b_answer"])
    if not (query_col and ans_a_col and ans_b_col):
        raise ValueError(f"Cannot find required columns. Found keys: {row_keys}. Need one query + two answers (e.g., user_input/answer_a/answer_b).")

    # Resume Logic
    # NOTE: For long_csv, we change from excel to csv to support streaming append.
    # We do this check once before looping repeats.
    if args.output_long_csv and args.output_long_csv.endswith(".xlsx"):
        print("[WARN] Output long file ends in .xlsx but streaming write supports CSV only. Changing extension to .csv for safety.")
        args.output_long_csv = args.output_long_csv.replace(".xlsx", ".csv")

    base_wide = args.output_wide_csv
    base_long = args.output_long_csv
    all_rows = df.to_dict("records")

    for repeat_idx in range(args.num_repeat):
        print(f"\n=== Starting Repeat Session {repeat_idx} (total {args.num_repeat}) ===")

        # æŠŠå½“å‰ repeat æ¬¡æ•°æŒ‚åˆ° args ä¸Šï¼Œä¾›ä¸‹æ¸¸è°ƒè¯•æ‰“å°ä½¿ç”¨
        setattr(args, "current_repeat_idx", repeat_idx)

        # Construct filenames for this repeatï¼šå¸¦ repeat çš„ CSV æ”¾åˆ°ä¸‹ä¸€çº§å­ç›®å½•ä¸­
        cur_wide_csv = make_repeat_path(base_wide, f"repeat{repeat_idx}")
        cur_long_csv = make_repeat_path(base_long, f"repeat{repeat_idx}")

        processed_indices = set()
        if os.path.exists(cur_wide_csv):
            try:
                # Only read columns needed to check index
                existing_df = pd.read_csv(cur_wide_csv, usecols=["_source_row_index"])
                processed_indices = set(existing_df["_source_row_index"].unique())
                print(f"[Info] Resuming: Found {len(processed_indices)} processed rows in {cur_wide_csv}")
            except Exception as e:
                print(f"[Warn] Could not read existing output file for resume: {e}. Starting fresh or appending blindly.")
        
        tasks_to_run = []
        for i, row in enumerate(all_rows):
            if i not in processed_indices:
                tasks_to_run.append((i, row))
        
        print(f"[Info] Repeat {repeat_idx}: Total rows: {len(all_rows)}. To process: {len(tasks_to_run)}.")

        # Determine if we need to write headers
        wide_header_needed = not os.path.exists(cur_wide_csv)
        long_header_needed = not os.path.exists(cur_long_csv)

        # å¯¹ CSV æ¨¡å¼åŒæ ·å¢åŠ åŸºäº LLM è°ƒç”¨æ¬¡æ•°çš„ç²¾ç»†è¿›åº¦æ¡ï¼š
        # æ¯è¡ŒåŒ…å« A/B ä¸¤ä¸ªå›ç­”ã€æ¯ä¸ªå›ç­”éœ€è¦ Ground / Structure å„ä¸€æ¬¡ â†’ ç†è®ºä¸Š 4 æ¬¡ LLM è°ƒç”¨/è¡Œ
        total_llm_calls_csv = len(tasks_to_run) * 4
        _progress_callback_csv, llm_pbar_csv = make_llm_progress(
            total_llm_calls_csv,
            f"LLM calls (Repeat {repeat_idx})",
        )

        with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
            # Submit all tasks
            future_to_idx = {
                executor.submit(
                    process_row,
                    idx,
                    row,
                    client,
                    args,
                    query_col,
                    ans_a_col,
                    ans_b_col,
                    _progress_callback_csv,
                ): idx
                for (idx, row) in tasks_to_run
            }

            # Iterate as they completeï¼ˆæ ·æœ¬çº§è¿›åº¦æ¡ï¼‰
            for future in tqdm.tqdm(
                concurrent.futures.as_completed(future_to_idx),
                total=len(tasks_to_run),
                desc=f"Judging (Repeat {repeat_idx}) [rows]",
            ):
                try:
                    idx, wide_res, long_res_list = future.result()
                    if wide_res is None:
                        continue  # Failed row

                    # Streaming Write - Wide
                    pd.DataFrame([wide_res]).to_csv(
                        cur_wide_csv,
                        mode='a',
                        header=wide_header_needed,
                        index=False,
                    )
                    wide_header_needed = False  # Only needed once

                    # Streaming Write - Long
                    if long_res_list:
                        pd.DataFrame(long_res_list).to_csv(
                            cur_long_csv,
                            mode='a',
                            header=long_header_needed,
                            index=False,
                        )
                        long_header_needed = False

                except Exception as exc:
                    idx = future_to_idx[future]
                    print(f"[Error] Exception for row {idx}: {exc}")

        llm_pbar_csv.close()

        print(f"[Done] Finished Repeat {repeat_idx}. Results saved to:\n  - {cur_wide_csv}\n  - {cur_long_csv}")

if __name__ == "__main__":
    main()

